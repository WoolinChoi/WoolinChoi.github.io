{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제2조 ① 대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "357\n",
      "357\n",
      "대한민국 의 국민 이 되는 요건 은 법률 로 정 한다 .\n",
      "354\n",
      "[102, 1, 22, 5, 111, 653, 4, 9, 24, 13, 6]\n",
      "187 1165\n",
      "i 0\n",
      "x (187, 187) \n",
      " [[  0   0   0 ...   0   0 102]\n",
      " [  0   0   0 ...   0 102  28]\n",
      " [  0   0   0 ... 102  28 602]\n",
      " ...\n",
      " [  0   0 102 ... 647 155   2]\n",
      " [  0 102  28 ... 155   2  20]\n",
      " [102  28 602 ...   2  20 180]]\n",
      "y (187, 1165) \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "i 1\n",
      "x (6, 187) \n",
      " [[  0   0   0 ...   0   0  45]\n",
      " [  0   0   0 ...   0  45 439]\n",
      " [  0   0   0 ...  45 439 648]\n",
      " [  0   0   0 ... 439 648 102]\n",
      " [  0   0   0 ... 648 102   4]\n",
      " [  0   0   0 ... 102   4 649]]\n",
      "y (6, 1165) \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "i 2\n",
      "x (12, 187) \n",
      " [[  0   0   0 ...   0   0 102]\n",
      " [  0   0   0 ...   0 102   1]\n",
      " [  0   0   0 ... 102   1 440]\n",
      " ...\n",
      " [  0   0   0 ...  34 651   4]\n",
      " [  0   0   0 ... 651   4  22]\n",
      " [  0   0   0 ...   4  22 331]]\n",
      "y (12, 1165) \n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(6917, 187)\n",
      "(6917, 187) (6917, 1165)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 187, 100)          116500    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1165)              117665    \n",
      "=================================================================\n",
      "Total params: 314,565\n",
      "Trainable params: 314,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      " - 26s - loss: 6.6716 - accuracy: 0.0304\n",
      "Epoch 2/500\n",
      " - 25s - loss: 5.8594 - accuracy: 0.0395\n",
      "Epoch 3/500\n",
      " - 30s - loss: 5.7565 - accuracy: 0.0450\n",
      "Epoch 4/500\n",
      " - 24s - loss: 5.7392 - accuracy: 0.0474\n",
      "Epoch 5/500\n",
      " - 28s - loss: 5.7210 - accuracy: 0.0461\n",
      "Epoch 6/500\n",
      " - 28s - loss: 5.6967 - accuracy: 0.0496\n",
      "Epoch 7/500\n",
      " - 25s - loss: 5.6685 - accuracy: 0.0510\n",
      "Epoch 8/500\n",
      " - 26s - loss: 5.6336 - accuracy: 0.0502\n",
      "Epoch 9/500\n",
      " - 27s - loss: 5.6001 - accuracy: 0.0541\n",
      "Epoch 10/500\n",
      " - 32s - loss: 5.5602 - accuracy: 0.0518\n",
      "Epoch 11/500\n",
      " - 31s - loss: 5.5211 - accuracy: 0.0532\n",
      "Epoch 12/500\n",
      " - 27s - loss: 5.4816 - accuracy: 0.0539\n",
      "Epoch 13/500\n",
      " - 25s - loss: 5.4464 - accuracy: 0.0529\n",
      "Epoch 14/500\n",
      " - 27s - loss: 5.4126 - accuracy: 0.0531\n",
      "Epoch 15/500\n",
      " - 36s - loss: 5.3795 - accuracy: 0.0539\n",
      "Epoch 16/500\n",
      " - 30s - loss: 5.3505 - accuracy: 0.0557\n",
      "Epoch 17/500\n",
      " - 26s - loss: 5.3137 - accuracy: 0.0604\n",
      "Epoch 18/500\n",
      " - 26s - loss: 5.2823 - accuracy: 0.0574\n",
      "Epoch 19/500\n",
      " - 27s - loss: 5.2481 - accuracy: 0.0606\n",
      "Epoch 20/500\n",
      " - 27s - loss: 5.2151 - accuracy: 0.0629\n",
      "Epoch 21/500\n",
      " - 28s - loss: 5.1860 - accuracy: 0.0666\n",
      "Epoch 22/500\n",
      " - 27s - loss: 5.1414 - accuracy: 0.0742\n",
      "Epoch 23/500\n",
      " - 26s - loss: 5.1144 - accuracy: 0.0788\n",
      "Epoch 24/500\n",
      " - 26s - loss: 5.0803 - accuracy: 0.0815\n",
      "Epoch 25/500\n",
      " - 25s - loss: 5.0451 - accuracy: 0.0820\n",
      "Epoch 26/500\n",
      " - 25s - loss: 5.0102 - accuracy: 0.0859\n",
      "Epoch 27/500\n",
      " - 25s - loss: 4.9809 - accuracy: 0.0932\n",
      "Epoch 28/500\n",
      " - 25s - loss: 4.9497 - accuracy: 0.0970\n",
      "Epoch 29/500\n",
      " - 25s - loss: 4.9192 - accuracy: 0.0957\n",
      "Epoch 30/500\n",
      " - 25s - loss: 4.8931 - accuracy: 0.0980\n",
      "Epoch 31/500\n",
      " - 25s - loss: 4.8576 - accuracy: 0.1060\n",
      "Epoch 32/500\n",
      " - 25s - loss: 4.8275 - accuracy: 0.1093\n",
      "Epoch 33/500\n",
      " - 25s - loss: 4.8018 - accuracy: 0.1174\n",
      "Epoch 34/500\n",
      " - 25s - loss: 4.7665 - accuracy: 0.1214\n",
      "Epoch 35/500\n",
      " - 25s - loss: 4.7390 - accuracy: 0.1235\n",
      "Epoch 36/500\n",
      " - 25s - loss: 4.7013 - accuracy: 0.1240\n",
      "Epoch 37/500\n",
      " - 25s - loss: 4.6858 - accuracy: 0.1327\n",
      "Epoch 38/500\n",
      " - 25s - loss: 4.6448 - accuracy: 0.1363\n",
      "Epoch 39/500\n",
      " - 25s - loss: 4.6227 - accuracy: 0.1420\n",
      "Epoch 40/500\n",
      " - 25s - loss: 4.5900 - accuracy: 0.1470\n",
      "Epoch 41/500\n",
      " - 25s - loss: 4.5644 - accuracy: 0.1485\n",
      "Epoch 42/500\n",
      " - 25s - loss: 4.5344 - accuracy: 0.1600\n",
      "Epoch 43/500\n",
      " - 25s - loss: 4.4961 - accuracy: 0.1680\n",
      "Epoch 44/500\n",
      " - 25s - loss: 4.4784 - accuracy: 0.1595\n",
      "Epoch 45/500\n",
      " - 25s - loss: 4.4466 - accuracy: 0.1661\n",
      "Epoch 46/500\n",
      " - 25s - loss: 4.4153 - accuracy: 0.1652\n",
      "Epoch 47/500\n",
      " - 25s - loss: 4.3945 - accuracy: 0.1719\n",
      "Epoch 48/500\n",
      " - 25s - loss: 4.3634 - accuracy: 0.1772\n",
      "Epoch 49/500\n",
      " - 25s - loss: 4.3384 - accuracy: 0.1811\n",
      "Epoch 50/500\n",
      " - 25s - loss: 4.3108 - accuracy: 0.1849\n",
      "Epoch 51/500\n",
      " - 25s - loss: 4.2789 - accuracy: 0.1913\n",
      "Epoch 52/500\n",
      " - 25s - loss: 4.2599 - accuracy: 0.1960\n",
      "Epoch 53/500\n",
      " - 25s - loss: 4.2194 - accuracy: 0.1923\n",
      "Epoch 54/500\n",
      " - 25s - loss: 4.2018 - accuracy: 0.1955\n",
      "Epoch 55/500\n",
      " - 25s - loss: 4.1799 - accuracy: 0.2050\n",
      "Epoch 56/500\n",
      " - 25s - loss: 4.1575 - accuracy: 0.2043\n",
      "Epoch 57/500\n",
      " - 25s - loss: 4.1229 - accuracy: 0.2141\n",
      "Epoch 58/500\n",
      " - 25s - loss: 4.0922 - accuracy: 0.2160\n",
      "Epoch 59/500\n",
      " - 25s - loss: 4.0743 - accuracy: 0.2180\n",
      "Epoch 60/500\n",
      " - 25s - loss: 4.0415 - accuracy: 0.2247\n",
      "Epoch 61/500\n",
      " - 25s - loss: 4.0327 - accuracy: 0.2260\n",
      "Epoch 62/500\n",
      " - 25s - loss: 4.0000 - accuracy: 0.2281\n",
      "Epoch 63/500\n",
      " - 25s - loss: 3.9838 - accuracy: 0.2276\n",
      "Epoch 64/500\n",
      " - 25s - loss: 3.9431 - accuracy: 0.2323\n",
      "Epoch 65/500\n",
      " - 25s - loss: 3.9266 - accuracy: 0.2410\n",
      "Epoch 66/500\n",
      " - 25s - loss: 3.8940 - accuracy: 0.2391\n",
      "Epoch 67/500\n",
      " - 25s - loss: 3.8770 - accuracy: 0.2442\n",
      "Epoch 68/500\n",
      " - 25s - loss: 3.8590 - accuracy: 0.2462\n",
      "Epoch 69/500\n",
      " - 25s - loss: 3.8335 - accuracy: 0.2504\n",
      "Epoch 70/500\n",
      " - 25s - loss: 3.8070 - accuracy: 0.2531\n",
      "Epoch 71/500\n",
      " - 25s - loss: 3.7892 - accuracy: 0.2543\n",
      "Epoch 72/500\n",
      " - 25s - loss: 3.7715 - accuracy: 0.2550\n",
      "Epoch 73/500\n",
      " - 25s - loss: 3.7341 - accuracy: 0.2624\n",
      "Epoch 74/500\n",
      " - 25s - loss: 3.7261 - accuracy: 0.2659\n",
      "Epoch 75/500\n",
      " - 25s - loss: 3.6999 - accuracy: 0.2716\n",
      "Epoch 76/500\n",
      " - 25s - loss: 3.6860 - accuracy: 0.2696\n",
      "Epoch 77/500\n",
      " - 25s - loss: 3.6555 - accuracy: 0.2711\n",
      "Epoch 78/500\n",
      " - 26s - loss: 3.6405 - accuracy: 0.2793\n",
      "Epoch 79/500\n",
      " - 26s - loss: 3.6259 - accuracy: 0.2797\n",
      "Epoch 80/500\n",
      " - 26s - loss: 3.6043 - accuracy: 0.2754\n",
      "Epoch 81/500\n",
      " - 25s - loss: 3.5753 - accuracy: 0.2868\n",
      "Epoch 82/500\n",
      " - 25s - loss: 3.5572 - accuracy: 0.2884\n",
      "Epoch 83/500\n",
      " - 25s - loss: 3.5400 - accuracy: 0.2939\n",
      "Epoch 84/500\n",
      " - 25s - loss: 3.5069 - accuracy: 0.2954\n",
      "Epoch 85/500\n",
      " - 25s - loss: 3.4927 - accuracy: 0.2930\n",
      "Epoch 86/500\n",
      " - 25s - loss: 3.4809 - accuracy: 0.3029\n",
      "Epoch 87/500\n",
      " - 25s - loss: 3.4573 - accuracy: 0.2987\n",
      "Epoch 88/500\n",
      " - 25s - loss: 3.4296 - accuracy: 0.3089\n",
      "Epoch 89/500\n",
      " - 25s - loss: 3.4244 - accuracy: 0.3030\n",
      "Epoch 90/500\n",
      " - 26s - loss: 3.4055 - accuracy: 0.3113\n",
      "Epoch 91/500\n",
      " - 26s - loss: 3.3778 - accuracy: 0.3143\n",
      "Epoch 92/500\n",
      " - 26s - loss: 3.3635 - accuracy: 0.3137\n",
      "Epoch 93/500\n",
      " - 26s - loss: 3.3533 - accuracy: 0.3159\n",
      "Epoch 94/500\n",
      " - 25s - loss: 3.3208 - accuracy: 0.3159\n",
      "Epoch 95/500\n",
      " - 25s - loss: 3.3185 - accuracy: 0.3201\n",
      "Epoch 96/500\n",
      " - 25s - loss: 3.2966 - accuracy: 0.3230\n",
      "Epoch 97/500\n",
      " - 26s - loss: 3.2738 - accuracy: 0.3328\n",
      "Epoch 98/500\n",
      " - 27s - loss: 3.2607 - accuracy: 0.3286\n",
      "Epoch 99/500\n",
      " - 27s - loss: 3.2501 - accuracy: 0.3247\n",
      "Epoch 100/500\n",
      " - 26s - loss: 3.2210 - accuracy: 0.3328\n",
      "Epoch 101/500\n",
      " - 26s - loss: 3.1942 - accuracy: 0.3410\n",
      "Epoch 102/500\n",
      " - 26s - loss: 3.1849 - accuracy: 0.3338\n",
      "Epoch 103/500\n",
      " - 26s - loss: 3.1626 - accuracy: 0.3406\n",
      "Epoch 104/500\n",
      " - 26s - loss: 3.1627 - accuracy: 0.3426\n",
      "Epoch 105/500\n",
      " - 26s - loss: 3.1392 - accuracy: 0.3421\n",
      "Epoch 106/500\n",
      " - 26s - loss: 3.1214 - accuracy: 0.3532\n",
      "Epoch 107/500\n",
      " - 25s - loss: 3.1082 - accuracy: 0.3497\n",
      "Epoch 108/500\n",
      " - 25s - loss: 3.0872 - accuracy: 0.3541\n",
      "Epoch 109/500\n",
      " - 26s - loss: 3.0749 - accuracy: 0.3539\n",
      "Epoch 110/500\n",
      " - 26s - loss: 3.0513 - accuracy: 0.3572\n",
      "Epoch 111/500\n",
      " - 26s - loss: 3.0384 - accuracy: 0.3616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      " - 26s - loss: 3.0298 - accuracy: 0.3626\n",
      "Epoch 113/500\n",
      " - 26s - loss: 2.9959 - accuracy: 0.3675\n",
      "Epoch 114/500\n",
      " - 26s - loss: 2.9850 - accuracy: 0.3701\n",
      "Epoch 115/500\n",
      " - 26s - loss: 2.9766 - accuracy: 0.3705\n",
      "Epoch 116/500\n",
      " - 26s - loss: 2.9581 - accuracy: 0.3731\n",
      "Epoch 117/500\n",
      " - 26s - loss: 2.9474 - accuracy: 0.3749\n",
      "Epoch 118/500\n",
      " - 26s - loss: 2.9253 - accuracy: 0.3812\n",
      "Epoch 119/500\n",
      " - 26s - loss: 2.9089 - accuracy: 0.3781\n",
      "Epoch 120/500\n",
      " - 26s - loss: 2.8927 - accuracy: 0.3763\n",
      "Epoch 121/500\n",
      " - 26s - loss: 2.8983 - accuracy: 0.3805\n",
      "Epoch 122/500\n",
      " - 26s - loss: 2.8666 - accuracy: 0.3889\n",
      "Epoch 123/500\n",
      " - 26s - loss: 2.8553 - accuracy: 0.3895\n",
      "Epoch 124/500\n",
      " - 26s - loss: 2.8451 - accuracy: 0.3892\n",
      "Epoch 125/500\n",
      " - 26s - loss: 2.8315 - accuracy: 0.3903\n",
      "Epoch 126/500\n",
      " - 26s - loss: 2.8206 - accuracy: 0.3919\n",
      "Epoch 127/500\n",
      " - 26s - loss: 2.7969 - accuracy: 0.3971\n",
      "Epoch 128/500\n",
      " - 26s - loss: 2.7871 - accuracy: 0.3979\n",
      "Epoch 129/500\n",
      " - 26s - loss: 2.7539 - accuracy: 0.4015\n",
      "Epoch 130/500\n",
      " - 26s - loss: 2.7399 - accuracy: 0.4073\n",
      "Epoch 131/500\n",
      " - 26s - loss: 2.7490 - accuracy: 0.4051\n",
      "Epoch 132/500\n",
      " - 26s - loss: 2.7301 - accuracy: 0.4074\n",
      "Epoch 133/500\n",
      " - 26s - loss: 2.7181 - accuracy: 0.4120\n",
      "Epoch 134/500\n",
      " - 26s - loss: 2.6966 - accuracy: 0.4159\n",
      "Epoch 135/500\n",
      " - 26s - loss: 2.6923 - accuracy: 0.4139\n",
      "Epoch 136/500\n",
      " - 26s - loss: 2.6841 - accuracy: 0.4110\n",
      "Epoch 137/500\n",
      " - 26s - loss: 2.6631 - accuracy: 0.4152\n",
      "Epoch 138/500\n",
      " - 26s - loss: 2.6488 - accuracy: 0.4208\n",
      "Epoch 139/500\n",
      " - 26s - loss: 2.6378 - accuracy: 0.4220\n",
      "Epoch 140/500\n",
      " - 26s - loss: 2.6149 - accuracy: 0.4188\n",
      "Epoch 141/500\n",
      " - 26s - loss: 2.6096 - accuracy: 0.4259\n",
      "Epoch 142/500\n",
      " - 26s - loss: 2.6023 - accuracy: 0.4298\n",
      "Epoch 143/500\n",
      " - 26s - loss: 2.5978 - accuracy: 0.4226\n",
      "Epoch 144/500\n",
      " - 26s - loss: 2.5621 - accuracy: 0.4324\n",
      "Epoch 145/500\n",
      " - 26s - loss: 2.5594 - accuracy: 0.4394\n",
      "Epoch 146/500\n",
      " - 26s - loss: 2.5434 - accuracy: 0.4343\n",
      "Epoch 147/500\n",
      " - 26s - loss: 2.5323 - accuracy: 0.4424\n",
      "Epoch 148/500\n",
      " - 26s - loss: 2.5124 - accuracy: 0.4469\n",
      "Epoch 149/500\n",
      " - 26s - loss: 2.5148 - accuracy: 0.4412\n",
      "Epoch 150/500\n",
      " - 26s - loss: 2.4918 - accuracy: 0.4448\n",
      "Epoch 151/500\n",
      " - 26s - loss: 2.4834 - accuracy: 0.4479\n",
      "Epoch 152/500\n",
      " - 26s - loss: 2.4749 - accuracy: 0.4516\n",
      "Epoch 153/500\n",
      " - 26s - loss: 2.4635 - accuracy: 0.4516\n",
      "Epoch 154/500\n",
      " - 26s - loss: 2.4468 - accuracy: 0.4527\n",
      "Epoch 155/500\n",
      " - 26s - loss: 2.4428 - accuracy: 0.4511\n",
      "Epoch 156/500\n",
      " - 25s - loss: 2.4123 - accuracy: 0.4619\n",
      "Epoch 157/500\n",
      " - 25s - loss: 2.4087 - accuracy: 0.4599\n",
      "Epoch 158/500\n",
      " - 25s - loss: 2.3969 - accuracy: 0.4628\n",
      "Epoch 159/500\n",
      " - 25s - loss: 2.3952 - accuracy: 0.4660\n",
      "Epoch 160/500\n",
      " - 26s - loss: 2.3817 - accuracy: 0.4629\n",
      "Epoch 161/500\n",
      " - 25s - loss: 2.3834 - accuracy: 0.4667\n",
      "Epoch 162/500\n",
      " - 25s - loss: 2.3469 - accuracy: 0.4749\n",
      "Epoch 163/500\n",
      " - 25s - loss: 2.3494 - accuracy: 0.4807\n",
      "Epoch 164/500\n",
      " - 25s - loss: 2.3368 - accuracy: 0.4785\n",
      "Epoch 165/500\n",
      " - 25s - loss: 2.3304 - accuracy: 0.4768\n",
      "Epoch 166/500\n",
      " - 25s - loss: 2.3091 - accuracy: 0.4830\n",
      "Epoch 167/500\n",
      " - 25s - loss: 2.2881 - accuracy: 0.4817\n",
      "Epoch 168/500\n",
      " - 25s - loss: 2.2947 - accuracy: 0.4836\n",
      "Epoch 169/500\n",
      " - 25s - loss: 2.2867 - accuracy: 0.4811\n",
      "Epoch 170/500\n",
      " - 25s - loss: 2.2596 - accuracy: 0.4915\n",
      "Epoch 171/500\n",
      " - 25s - loss: 2.2686 - accuracy: 0.4855\n",
      "Epoch 172/500\n",
      " - 26s - loss: 2.2543 - accuracy: 0.4920\n",
      "Epoch 173/500\n",
      " - 25s - loss: 2.2290 - accuracy: 0.5024\n",
      "Epoch 174/500\n",
      " - 25s - loss: 2.2335 - accuracy: 0.4939\n",
      "Epoch 175/500\n",
      " - 25s - loss: 2.2237 - accuracy: 0.4959\n",
      "Epoch 176/500\n",
      " - 25s - loss: 2.2024 - accuracy: 0.5040\n",
      "Epoch 177/500\n",
      " - 25s - loss: 2.1846 - accuracy: 0.5119\n",
      "Epoch 178/500\n",
      " - 25s - loss: 2.1811 - accuracy: 0.5053\n",
      "Epoch 179/500\n",
      " - 25s - loss: 2.1779 - accuracy: 0.5073\n",
      "Epoch 180/500\n",
      " - 25s - loss: 2.1714 - accuracy: 0.5074\n",
      "Epoch 181/500\n",
      " - 25s - loss: 2.1451 - accuracy: 0.5131\n",
      "Epoch 182/500\n",
      " - 25s - loss: 2.1444 - accuracy: 0.5099\n",
      "Epoch 183/500\n",
      " - 25s - loss: 2.1308 - accuracy: 0.5160\n",
      "Epoch 184/500\n",
      " - 26s - loss: 2.1374 - accuracy: 0.5180\n",
      "Epoch 185/500\n",
      " - 25s - loss: 2.1040 - accuracy: 0.5181\n",
      "Epoch 186/500\n",
      " - 25s - loss: 2.1157 - accuracy: 0.5177\n",
      "Epoch 187/500\n",
      " - 25s - loss: 2.0971 - accuracy: 0.5179\n",
      "Epoch 188/500\n",
      " - 25s - loss: 2.0964 - accuracy: 0.5260\n",
      "Epoch 189/500\n",
      " - 25s - loss: 2.0820 - accuracy: 0.5254\n",
      "Epoch 190/500\n",
      " - 25s - loss: 2.0747 - accuracy: 0.5233\n",
      "Epoch 191/500\n",
      " - 25s - loss: 2.0567 - accuracy: 0.5296\n",
      "Epoch 192/500\n",
      " - 25s - loss: 2.0561 - accuracy: 0.5369\n",
      "Epoch 193/500\n",
      " - 25s - loss: 2.0332 - accuracy: 0.5385\n",
      "Epoch 194/500\n",
      " - 25s - loss: 2.0257 - accuracy: 0.5358\n",
      "Epoch 195/500\n",
      " - 25s - loss: 2.0242 - accuracy: 0.5416\n",
      "Epoch 196/500\n",
      " - 26s - loss: 2.0279 - accuracy: 0.5404\n",
      "Epoch 197/500\n",
      " - 25s - loss: 2.0120 - accuracy: 0.5381\n",
      "Epoch 198/500\n",
      " - 25s - loss: 1.9884 - accuracy: 0.5495\n",
      "Epoch 199/500\n",
      " - 25s - loss: 1.9842 - accuracy: 0.5432\n",
      "Epoch 200/500\n",
      " - 25s - loss: 1.9835 - accuracy: 0.5478\n",
      "Epoch 201/500\n",
      " - 25s - loss: 1.9736 - accuracy: 0.5432\n",
      "Epoch 202/500\n",
      " - 37s - loss: 1.9582 - accuracy: 0.5513\n",
      "Epoch 203/500\n",
      " - 45s - loss: 1.9535 - accuracy: 0.5497\n",
      "Epoch 204/500\n",
      " - 49s - loss: 1.9445 - accuracy: 0.5566\n",
      "Epoch 205/500\n",
      " - 47s - loss: 1.9452 - accuracy: 0.5531\n",
      "Epoch 206/500\n",
      " - 46s - loss: 1.9257 - accuracy: 0.5624\n",
      "Epoch 207/500\n",
      " - 45s - loss: 1.8986 - accuracy: 0.5617\n",
      "Epoch 208/500\n",
      " - 45s - loss: 1.9189 - accuracy: 0.5638\n",
      "Epoch 209/500\n",
      " - 46s - loss: 1.9064 - accuracy: 0.5621\n",
      "Epoch 210/500\n",
      " - 45s - loss: 1.8873 - accuracy: 0.5612\n",
      "Epoch 211/500\n",
      " - 41s - loss: 1.8767 - accuracy: 0.5659\n",
      "Epoch 212/500\n",
      " - 45s - loss: 1.8848 - accuracy: 0.5751\n",
      "Epoch 213/500\n",
      " - 45s - loss: 1.8858 - accuracy: 0.5674\n",
      "Epoch 214/500\n",
      " - 45s - loss: 1.8709 - accuracy: 0.5696\n",
      "Epoch 215/500\n",
      " - 46s - loss: 1.8683 - accuracy: 0.5728\n",
      "Epoch 216/500\n",
      " - 47s - loss: 1.8525 - accuracy: 0.5742\n",
      "Epoch 217/500\n",
      " - 46s - loss: 1.8343 - accuracy: 0.5858\n",
      "Epoch 218/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8f9b12624e11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;31m###### Training #########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from konlpy.corpus import kolaw\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "c = kolaw.open('constitution.txt').read()\n",
    "senstents = [s for s in sent_tokenize(c)]\n",
    "\n",
    "print(senstents[3])  # 제2조① 대한민국의 국민이 되는 요건은 법률로 정한다.\n",
    "print(len(senstents))  # 357\n",
    "\n",
    "\n",
    "########## 전처리 ###################\n",
    "twitter = Okt()\n",
    "doc0 = [\" \".join([\"\".join(w) for w, t in twitter.pos(s)\n",
    "                  if t not in ['Number', \"Foreign\"] and w not in [\"제\", \"조\"]]) for s in sent_tokenize(c)]\n",
    "print(len(doc0))  # 357\n",
    "print(doc0[3])  # 대한민국 의 국민 이 되는 요건 은 법률 로 정 한다 .\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(doc0)\n",
    "\n",
    "doc = [l for l in tokenizer.texts_to_sequences(doc0) if len(l) > 1]\n",
    "print(len(doc))  # 354\n",
    "print(doc[3])  # [102, 1, 22, 5, 111, 653, 4, 9, 24, 13, 6]\n",
    "\n",
    "maxlen = max([len(x) - 1 for x in doc])     # 187\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 1165\n",
    "\n",
    "print(maxlen, vocab_size) #187 1165\n",
    "\n",
    "############ Data Generation ##################\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(X, maxlen, vocab_size):\n",
    "    for sentence in X:\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(1, len(sentence)):\n",
    "            inputs.append(sentence[0:i])\n",
    "            targets.append(sentence[i])\n",
    "        y = np_utils.to_categorical(targets, vocab_size)  # 원핫 인코딩\n",
    "\n",
    "        inputs_sequence = sequence.pad_sequences(inputs, maxlen=maxlen)  # 최대 크기\n",
    "        yield (inputs_sequence, y)\n",
    "\n",
    "for i, (x, y) in enumerate(generate_data(doc, maxlen, vocab_size)):\n",
    "    print(\"i\", i)\n",
    "    print(\"x\", x.shape, \"\\n\", x) # 예---  (12, 187)   [[  0   0   0 ...   0   0 102] [  0   0   0 ...   0 102   1]\n",
    "    print(\"y\", y.shape, \"\\n\", y) # 예---  (12, 1165) [[0. 1. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for x, y in generate_data(doc, maxlen, vocab_size):\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "print (type(X))  # <class 'list'>\n",
    "X = np.concatenate(X)\n",
    "print (type(X))  # <class 'numpy.ndarray'>\n",
    "print (X.shape)  # (6917, 187)\n",
    "\n",
    "Y = np.concatenate(Y)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "\n",
    "######## Model ###########\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=maxlen))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "###### Training #########\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=[\"accuracy\"])\n",
    "hist = model.fit(X, Y, epochs=500, batch_size=800, verbose=2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.show()\n",
    "\n",
    "## 모델 저장 ##\n",
    "model.save(\"rnn_text_gen.hdf5\")\n",
    "\n",
    "## 모델 로드 ##\n",
    "from keras.models import load_model\n",
    "model = load_model(\"rnn_text_gen.hdf5\")\n",
    "\n",
    "###### predict_word #########\n",
    "word_list = '대한민국 의 국민 이 되는 요건 은 법률 로 정한 다 .'.split(\" \")\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "x = sequence.pad_sequences([[tokenizer.word_index[w] for w in word_list[:2]]], maxlen=maxlen)\n",
    "p = model.predict(x)[0]\n",
    "idx = np.flip(np.argsort(p), 0)\n",
    "\n",
    "for i in idx[:5]:\n",
    "    print(reverse_word_map[i])\n",
    "\n",
    "def predict_word(i, n=1):\n",
    "    x = sequence.pad_sequences([[tokenizer.word_index[w] for w in word_list[:i]]], maxlen=maxlen)\n",
    "    p = model.predict(x)[0]\n",
    "    print(\"p  \", p)\n",
    "    idx = np.flip(np.argsort(p), 0)\n",
    "    print(\"np.argsort(p)  \", np.argsort(p))\n",
    "    print(\"np.flip(np.argsort(p), 0)\",np.flip(np.argsort(p), 0))\n",
    "\n",
    "    for j in idx[:n]:\n",
    "        print('\"', \" \".join(word_list[:i]), '\"', reverse_word_map[j], \" (p={:4.2f}%)\".format(100 * p[j]))\n",
    "\n",
    "print (predict_word(1, n=3))\n",
    "print (predict_word(2, n=3))\n",
    "print (predict_word(3, n=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
