I"3;<h1 id="hive2"><a href="">hive2</a></h1>

<h2 id="학습">학습</h2>
<ul>
  <li>Hive 테이블 생성
    <ul>
      <li>내부 테이블</li>
      <li>외부 테이블</li>
    </ul>
  </li>
  <li>파티션 테이블
    <ul>
      <li>insert</li>
      <li>load</li>
      <li>외부테이블 파티션</li>
      <li>다중컬럼 파티션</li>
    </ul>
  </li>
  <li>SQOOP 설치</li>
</ul>

<h2 id="실습">실습</h2>
<ul>
  <li>hivedemo
    <ul>
      <li>접속
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) syetemctl status mariadb.service로 상태 확인 후
2) active가 안되있을 시 syetemctl restart mariadb.service
3) active가 되있으면 syetemctl start mariadb.service
</code></pre></div>        </div>
      </li>
      <li>백그라운드로 실행하기
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ hive --service metastore &amp;
</code></pre></div>        </div>
      </li>
      <li>백그라운드 실행 확인하기
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ ps -ef | grep metastore
</code></pre></div>        </div>
      </li>
      <li>하이브서버 백그라운드 실행하기
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ hive --service HiveServer2 &amp;
</code></pre></div>        </div>
      </li>
      <li>하이브 실행(hive)
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ hive
</code></pre></div>        </div>
      </li>
      <li>하이브 실행(beeline)
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ beeline
2) beeline&gt; !connect jdbc:hive2://
3) Enter username for jdbc:hive2://: hive
4) Enter password for jdbc:hive2://: 1234
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>u.item
    <ul>
      <li>생성
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (default)&gt; show databases;
2) hive (default)&gt; create database hivedemo;
3) hive (default)&gt; use hivedemo;
</code></pre></div>        </div>
      </li>
      <li>샘플 예문 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ hdfs dfs -ls -R /user/hadoop (입력된 데이터를 저장하는 곳)
2) [hadoop@dn01 ~]$ hdfs dfs -ls -R /user/hive/warehouse (hive가 기본적으로 저장하는 곳)
</code></pre></div>        </div>
      </li>
      <li>테이블 만들기
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE movies (
movie_id INT,
movie_title STRING,
release_date STRING,
video_release_date STRING,
imdb_url STRING,
unknown INT,
action INT,
adventure INT,
animation INT,
children INT,
comedy INT,
crime INT,
documentary INT,
drama INT,
fantasy INT,
film_noir INT,
horror INT,
musical INT,
mystery INT,
romance INT,
sci_fi INT,
thriller INT,
war INT,
Western INT
)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;
</code></pre></div>        </div>
      </li>
      <li>테이블 로드
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data inpath '/user/hadoop/movies/u.item/' into table movies;
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>테이블 삭제
    <ul>
      <li>truncate
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; truncate table movies; (삭제)
2) hive (hivedemo)&gt; select * from movies; (확인)
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/movies (확인)
</code></pre></div>        </div>
      </li>
      <li>drop
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data local inpath '/home/hadoop/hive_data/ml-100k/u.item' into table movies; (재테이블로드)
2) [hadoop@dn01 ~]$ ll /home/hadoop/hive_data/ml-100k (파일이동되었나 확인)
3) hive (hivedemo)&gt; drop table movies; (삭제)
4) hive (hivedemo)&gt; select * from movies; (확인)
5) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/ (확인)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>u.user
    <ul>
      <li>데이터 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/userinfo
</code></pre></div>        </div>
      </li>
      <li>테이블 생성
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE EXTERNAL TABLE users (
user_id INT,
age INT,
gender STRING,
occupation STRING,
zip_code STRING
)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;
</code></pre></div>        </div>
      </li>
      <li>테이블 로드 및 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data inpath '/user/hadoop/userinfo/u.user' into table users;
2) hive (hivedemo)&gt; select * from users;
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/userinfo
4) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users
</code></pre></div>        </div>
      </li>
      <li>테이블 drop 후 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; drop table users;
2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users (외부테이블이라 파일이 안지워짐)
</code></pre></div>        </div>
      </li>
      <li>테이블2 생성
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE EXTERNAL TABLE users (
user_id INT,
age INT,
gender STRING,
occupation STRING,
zip_code STRING
)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
LOCATION '/user/hadoop/hive_data/userinfo2';
</code></pre></div>        </div>
      </li>
      <li>테이블 로드 및 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data local inpath '/home/hadoop/hive_data/ml-100k/u.user' into table users;
2) hive (hivedemo)&gt; select * from users;
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/userinfo
4) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users
</code></pre></div>        </div>
      </li>
      <li>테이블 drop 후 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; drop table users;
2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users (안지워짐)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>partition
    <ul>
      <li>orders 테이블 생성
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create table orders
(
id string,
customer_id string,
product_id string,
quantity int,
amount double,
zipcode char(5)
)
partitioned by (state char(2))
row format delimited fields terminated by ',';
1) hive (hivedemo)&gt; desc formatted orders;
</code></pre></div>        </div>
      </li>
      <li>데이터 파일 만들기
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1<span class="o">)</span> <span class="o">[</span>hadoop@dn01 hive_data]<span class="nv">$ </span>vi orders_CA.csv
<span class="c">########## orders_CA.csv ##########</span>
o1,c1,p1,1,379.99,90210
o2,c2,p2,1,8.99,90002
o3,c3,p3,1,100,94305
o4,c4,p4,1,669,94304
<span class="c">########## orders_CA.csv ##########</span>
2<span class="o">)</span> <span class="o">[</span>hadoop@dn01 hive_data]<span class="nv">$ </span>vi orders_CT.csv
<span class="c">########## orders_CT.csv ##########</span>
o11,c100,p101,1,379.99,06010
o22,c200,p201,1,8.99,06911
<span class="c">########## orders_CT.csv ##########</span>
</code></pre></div>        </div>
      </li>
      <li>테이블 로드 및 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data local inpath '/home/hadoop/hive_data/orders_CA.csv' into table orders partition(state='CA');
2) hive (hivedemo)&gt; select * from orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/orders (파티션은 서브디렉터리로 만들어짐)
</code></pre></div>        </div>
      </li>
      <li>테이블 로드 및 확인2
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data local inpath '/home/hadoop/hive_data/orders_CT.csv' into table orders partition(state='CT');
2) hive (hivedemo)&gt; select * from orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/orders
</code></pre></div>        </div>
      </li>
      <li>all_orders 테이블 생성
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create table all_orders
(
id string,
customer_id string,
product_id string,
quantity int,
amount double,
postalcode string
)
partitioned by (country string, state string)
row format delimited fields terminated by ',';
</code></pre></div>        </div>
      </li>
      <li>테이블 로드 및 확인
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data local inpath '/home/hadoop/hive_data/orders_CA.csv' into table all_orders partition(country='US', state='CA');
2) hive (hivedemo)&gt; select * from all_orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/all_orders/county=US (서브디렉터리가 2개 생성)
</code></pre></div>        </div>
      </li>
      <li>테이블 로드 및 확인2
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) hive (hivedemo)&gt; load data local inpath '/home/hadoop/hive_data/orders_CT.csv' into table all_orders partition(country='KR', state='CT');
2) hive (hivedemo)&gt; select * from all_orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/all_orders/county=KR
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>sqoop
    <ul>
      <li>설치
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [root@dn01 ~]# cd /tmp
2) [root@dn01 tmp]# wget http://mirror.apache-kr.org/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
3) [root@dn01 tmp]# tar -xvzpf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
4) [root@dn01 tmp]# mkdir -p /opt/sqoop/1.4.7
5) [root@dn01 tmp]# mv sqoop-1.4.7.bin__hadoop-2.6.0/* /opt/sqoop/1.4.7/
6) [root@dn01 tmp]# ln -s /opt/sqoop/1.4.7 /opt/sqoop/current
</code></pre></div>        </div>
      </li>
      <li>mysql
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) [root@dn01 tmp]# tar -xvzpf mysql-connector-java-5.1.38.tar.gz
2) [root@dn01 tmp]# cd mysql-connector-java-5.1.38/
3) [root@dn01 mysql-connector-java-5.1.38]# cp mysql-connector-java-5.1.38-bin.jar /opt/sqoop/current/lib
4) [root@dn01 tmp]# chown hadoop:hadoop /opt/sqoop/current/lib/mysql-connector-java-5.1.38-bin.jar
</code></pre></div>        </div>
      </li>
      <li>환경변수
        <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1<span class="o">)</span> <span class="o">[</span>hadoop@dn01 ~]<span class="nv">$ </span>vi ~/.bash_profile
<span class="c">#### SQOOP 1.4.7 ##############</span>
<span class="nb">export </span><span class="nv">SQOOP_HOME</span><span class="o">=</span>/opt/sqoop/current
<span class="nb">export </span><span class="nv">SQOOP_PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$SQOOP_HOME</span>/bin
<span class="c">#### SQOOP 1.4.7 ##############</span>
2<span class="o">)</span> <span class="o">[</span>hadoop@dn01 ~]<span class="nv">$ </span><span class="nb">source</span> ~/.bash_profile
3<span class="o">)</span> <span class="o">[</span>hadoop@dn01 ~]<span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$SQOOP_HOME</span>
4<span class="o">)</span> <span class="o">[</span>hadoop@dn01 current]<span class="nv">$ </span><span class="nb">pwd
</span>5<span class="o">)</span> <span class="o">[</span>hadoop@dn01 current]<span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$SQOOP_HOME</span>/conf
6<span class="o">)</span> <span class="o">[</span>hadoop@dn01 conf]<span class="nv">$ </span><span class="nb">mv </span>sqoop-env-template.sh sqoop-env.s
7<span class="o">)</span> <span class="o">[</span>hadoop@dn01 conf]<span class="nv">$ </span>vi sqoop-env.sh
<span class="c">########## sqoop-env.sh ###########</span>
<span class="c">#Set path to where bin/hadoop is available</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span>/opt/hadoop/current
<span class="c">#Set path to where hadoop-*-core.jar is available</span>
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span>/opt/hadoop/current
<span class="c">#Set the path to where bin/hive is available</span>
<span class="nb">export </span><span class="nv">HIVE_HOME</span><span class="o">=</span>/opt/hive/current
<span class="c">########## sqoop-env.sh ###########</span>
8<span class="o">)</span> <span class="o">[</span>hadoop@dn01 conf]<span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$SQOOP_HOME</span>
9<span class="o">)</span> <span class="o">[</span>hadoop@dn01 current]<span class="nv">$ </span><span class="nb">cp </span>sqoop-1.4.7.jar <span class="nv">$HADOOP_HOME</span>/share/hadoop/tools/lib
10<span class="o">)</span> <span class="o">[</span>hadoop@dn01 current]<span class="nv">$ </span>ll <span class="nv">$HADOOP_HOME</span>/share/hadoop/tools/lib
11<span class="o">)</span> <span class="o">[</span>hadoop@dn01 current]<span class="nv">$ </span>sqoop-version <span class="o">(</span>설치확인<span class="o">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h2 id="정리">정리</h2>
<ul>
  <li>테이블 삭제
    <ul>
      <li>delete from 테이블명 : 데이터만 삭제후에 메모리 자리 남김(hive는 지원안함)</li>
      <li>drop table 테이블명 : 데이터 + 구조 삭제</li>
      <li>truncate 테이블명 : 데이터 삭제후에 메모리 반환</li>
      <li>hdfs는 메모리가 아니라 파일로 저장하는데 truncate 결과 확인 : 파일삭제</li>
    </ul>
  </li>
  <li>CREATE EXTERNAL TABLE
    <ul>
      <li>사용자가 실수로 테이블을 DROP 했더라도 데이터가 보존됨(메타데이터는 지워짐)</li>
    </ul>
  </li>
  <li>hive와 hdfs : 테이블 = 디렉터리, 파티션 = 서브디렉터리, 데이터 = 파일</li>
</ul>
:ET