---
layout: post
title: webdata
category: webconn
tags: [python, webconn]
comments: false
---

# [webdata]()

## requests
~~~python
"""
    파이썬에서 웹을 요청할 수 있는 라이브러리
        1- requests 라이브러리 (s붙음 주의) - 추가
        2- urllib 라이브러리 - 내장모듈

    차이점
        1- requests는 요청 메소드(get/post)를 구분하지만 urllib는 보내는 데이타 여부에 따라 구분됨
        2- 데이타 보낼 때 requests는 딕셔러니 형태로 urllib는 인코딩한 바이너리 형태로 보낸다
        
    requests 라이브러리 추가
    메뉴 > File > Settings > Project Interpreter > + 버튼 > 'requests' 검색 후 인스톨

[ requests 모듈 ]
(1) Rest API 지원
    import requests
    resp = requests.get('http://www.mywebsite.com/user')
    resp = requests.post('http://www.mywebsite.com/user')
    resp = requests.put('http://www.mywebsite.com/user/put')
    resp = requests.delete('http://www.mywebsite.com/user/delete')

(2) 파라미터가 딕셔너리 인수로 가능
    data = {'firstname':'John', 'lastname':'Kim', 'job':'baksu'}
    resp = requests.post('http://www.mywebsite.com/user', data=userdata)

(3) json 디코더 내장 (따로 json 모듈 사용 안해도 됨)
    resp.json()
"""

import requests as req
url = "http://www.google.com"
res = req.get(url)
# print("text:", res.text)
# print("content:", res.content)

headers = res.headers
print(headers)
# 키와 값으로 나눠서 출력
for keys, values in headers.items():
    print("keys:", keys)
    print("values:", values)
~~~

## urllib
~~~python
# import urllib.request
from urllib import request as req

url = "http://www.google.com"
site = req.urlopen(url)
# print(site)
page = site.read()
print("page:", page)
print("status:", site.status)
headers = site.getheaders()
print("headers:", headers)
# 헤더정보의 이름과 값으로 출력
for name, values in headers:
    print("name:", name)
    print("values:", values)
~~~

## retretrieve
~~~python
# 크롬에서 구글이미지 > 오른마우스 > 이미지 주소 복사 >
# 구글 이미지 : https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png
# 다음 이미지 : https://t1.daumcdn.net/daumtop_chanel/op/20170315064553027.png

"""
    urllib 라이브러리(패키지):
        - URL를 다루는 모듈을 모아 놓은 패키지
        - Http나 Ftp를 사용하여 데이터를 다운로드 할 때 사용하는 라이브러리

        [예] request 모듈 : 웹 요청을 보내고 받는 기능을 하는 모듈
                - urlretrieve() 함수를 이용하여 이미지를 다운로드 받아 파일로 저장한다.
"""

url = "https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png"
imgName = "./data/google.png"

# from urllib import request
# request.urlretrieve(url, imgName)

# (1)
# import urllib.request
# urllib.request.urlretrieve(url, imgName)

# (2)
# import urllib.request as req
# req.urlretrieve(url, imgName)
~~~

## urlopen
~~~python
# urlretrieve(): 파일로 바로 저장
# urlopen(): 파일로 바로 저장하기 않고 메모리에 로딩을 한다.

""" [참고] 파일저장 기본방식
    f = open('a.txt','w')
    f.write("테스트 내용")
    f.close()

    위의 과정을 with 문으로 간략하게 close 필요없음
    with open("a.txt","w") as f:
        f.write("테스트 내용")
"""

from urllib import request

url = "https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png"
imgName = "./data/google.png"

# 1. 웹에서 열기
site = request.urlopen(url)
img = site.read()

# 2. 파일 저장
with open(imgName, "wb") as f:    # 이미지는 t(텍스트)가 아닌 b(바이너리)
    f.write(img)
~~~

## urljoin
~~~python
"""
 urllib.parse.urljoin() : 상대경로를 절대경로로 변화하는 함수
"""

from urllib import parse
baseUrl = "http://www.example.com/html/a.html"

# url 부분 경로로를 바꾼다.
print(parse.urljoin(baseUrl, "b.html"))    # http://www.example.com/html/b.html
print(parse.urljoin(baseUrl, "/c.html"))    # http://www.example.com/c.html
print(parse.urljoin(baseUrl, "../sub/b.html"))    # http://www.example.com/sub/b.html
print(parse.urljoin(baseUrl, "/sub/e.html"))    # http://www.example.com/sub/e.html

# url 전체 경로를 바꾼다
print(parse.urljoin(baseUrl, "http://www.mysite.com/mypage.jsp"))    # http://www.mysite.com/mypage.jsp
~~~