---
layout: post
title: setting
category: hadoop
tags: [hadoop]
comments: false
---

# [setting]()

## 학습
* Hadoop 설정

## 실습
* 자바 및 Hadoop 환경 변수
    - 자바 및 Hadoop 환경 변수 추가(설정파일 수정할 때는 MultiExe 보다 하나씩 하는 것이 나을 수 있음)
    <br> vi ~/.bash_profile
    ~~~shell
    #### HADOOP 2.7.7 start ############
    PATH=$PATH:$HOME/bin
    export HADOOP_HOME=/opt/hadoop/current
    export PATH=$PATH:$HADOOP_HOME/bin
    export PATH=$PATH:$HADOOP_HOME/sbin
    #### HADOOP 2.7.7end############

    #### JAVA 1.8.0 start#############
    export JAVA_HOME=/opt/jdk/current
    export PATH=$PATH:$JAVA_HOME/bin
    #### JAVA 1.8.0 end##############
    ~~~
    <br> source ~/.bash_profile

    - 자바 및 Hadoop 버전 확인
    <br> java -version
    <br> hadoop version

    - 자바 확인
    <br> 1) vi Hello.java
    ~~~java
    public class Hello{
        public static void main(String[] args){
            System.out.println("Hello World!!!");
        }
    }
    ~~~
    <br> 2) javac Hello.java
    <br> 3) java Hello

* 키 설정
    - 비밀번호없이 각노드를 접속할 수 있도록 공개키 공유(SSH)
    <br> vi /etc/hosts(root 권한으로) - localhost 꼭 지운다(모든 노드 다)
    ~~~shell
    192.168.56.101 nn01
    192.168.56.102 dn01
    192.168.56.103 dn02
    ~~~

    - 키 만들기
    <br> 1) [hadoop@nn01 ~]$ ssh-keygen
    <br> 2) [hadoop@dn01 ~]$ ssh-keygen
    <br> 3) [hadoop@dn02 ~]$ ssh-keygen

    - 키 복사하기
    <br> 1) [hadoop@nn01 ~]$ ssh-copy-id hadoop@dn01
    <br> [hadoop@nn01 ~]$ ssh-copy-id hadoop@nn01
    <br> [hadoop@nn01 ~]$ ssh-copy-id hadoop@dn02
    <br> 2) [hadoop@dn01 ~]$ ssh-copy-id hadoop@dn01
    <br> [hadoop@dn01 ~]$ ssh-copy-id hadoop@nn01
    <br> [hadoop@dn01 ~]$ ssh-copy-id hadoop@dn02
    <br> 3) [hadoop@dn02 ~]$ ssh-copy-id hadoop@dn01
    <br> [hadoop@dn02 ~]$ ssh-copy-id hadoop@nn01
    <br> [hadoop@dn02 ~]$ ssh-copy-id hadoop@dn02

    - 패스워드없이 이동 가능(나올 때는 exit 또는 logout)
    <br> 1) [hadoop@nn01 ~]$ ssh dn01
    <br> [hadoop@nn01 ~]$ ssh dn02
    <br> 2) [hadoop@dn01 ~]$ ssh nn01
    <br> [hadoop@dn01 ~]$ ssh dn02
    <br> 3) [hadoop@dn02 ~]$ ssh nn01
    <br> [hadoop@dn02 ~]$ ssh dn01

* Hadoop 설정
    - core-site.xml 설정
    <br> [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/core-site.xml
    ~~~shell
    <configuration>
    <property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn01:9000</value>
    </property>
    </configuration>
    ~~~

    - hdfs-site.xml 설정
    <br> [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/hdfs-site.xml
    ~~~shell
    <configuration>
    <property>
    <name>dfs.replication</name>
    <value>1</value>
    </property>
    <property>
    <name>dfs.namenode.http-address</name>
    <value>nn01:50070</value>
    </property>
    <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>nn01:50090</value>
    </property>
    <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/home/hadoop/hadoop_data/hdfs/namenode</value>
    </property>
    <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/home/hadoop/hadoop_data/hdfs/datanode</value>
    </property>
    <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file:/home/hadoop/hadoop_data/hdfs/namesecondary</value>
    </property>
    <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
    </property>
    </configuration>
    ~~~

    - yarn-site.xml 설정
    <br> [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/yarn-site.xml
    ~~~shell
    <configuration>
    <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    </property>
    <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>nn01:8030</value>
    </property>
    <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>nn01:8031</value>
    </property>
    <property>
    <name>yarn.resourcemanager.address</name>
    <value>nn01:8032</value>
    </property>
    <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>nn01</value>
    </property>
    </configuration>
    ~~~

    - mapred-site.xml 설정
    <br> 1) [hadoop@nn01 ~]$ cp /opt/hadoop/current/etc/hadoop/mapred-site.xml.template /opt/hadoop/current/etc/hadoop/mapred-site.xml
    <br> 2) [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/mapred-site.xml
    ~~~shell
    <configuration>
    <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
    </property>
    <property>
    <name>mapreduce.jobtracker.hosts.exclude.filename</name>
    <value>$HADOOP_HOME/etc/hadoop/exclude</value>
    </property>
    <property>
    <name>mapreduce.jobtracker.hosts.filename</name>
    <value>$HADOOP_HOME/etc/hadoop/include</value>
    </property>
    </configuration>
    ~~~

    - Masters, Slaves 설정
    <br> 1) [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/masters
    ~~~shell
    nn01
    ~~~
    <br> 2) [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/slaves
    ~~~shell
    dn01
    dn02
    ~~~

    - hadoop-env.sh 설정
    <br> [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/hadoop-env.sh
    ~~~shell
    # The java implementation to use.
    export JAVA_HOME=/opt/jdk/current
    ~~~

    - yarn-env.sh 설정
    <br> [hadoop@nn01 ~]$ vi /opt/hadoop/current/etc/hadoop/yarn-env.sh
    ~~~shell
    # some Java parameters
    export JAVA_HOME=/opt/jdk/current
    ~~~

    - nn01 설정을 dn01와 dn02 복사하기
    <br> 1) [root@nn01 ~]# scp -r /opt/hadoop/* dn01:/opt/hadoop
    <br> 2) [root@nn01 ~]# scp -r /opt/hadoop/* dn02:/opt/hadoop
    <br> 3) [root@dn01 ~]# rm -rf /opt/hadoop/current
    <br> [root@dn01 ~]# ln -s /opt/hadoop/2.7.7 /opt/hadoop/current
    <br> [root@dn01 ~]# chown -R hadoop:hadoop /opt/hadoop/
    <br> 4) [root@dn02 ~]# rm -rf /opt/hadoop/current
    <br> [root@dn02 ~]# ln -s /opt/hadoop/2.7.7 /opt/hadoop/current
    <br> [root@dn02 ~]# chown -R hadoop:hadoop /opt/hadoop/

* Hadoop 확인
    - Hadoop namenode 디렉토리 생성(nn01 : Namenode)
    <br> 1) mkdir -p ~/hadoop_data/hdfs/namenode
    <br> 2) mkdir -p ~/hadoop_data/hdfs/namesecondary

    - Hadoop datanode 디렉토리 생성(dn01 : Datanode)
    <br> mkdir -p ~/hadoop_data/hdfs/datanode

    - Hadoop datanode 디렉토리 생성(dn02 : Datanode)
    <br> mkdir -p ~/hadoop_data/hdfs/datanode

    - Namenode 포맷(nn01)
    <br> hadoop namenode -format

    - Daemon 시작(nn01)
    <br> start-all.sh

    - 서비스 정상 확인
    <br> 1) [hadoop@nn01 ~]$ jps
    <br> 2) [hadoop@dn01 ~]$ jps
    <br> 3) [hadoop@dn02 ~]$ jps

    - GUI(윈도우 브라우저로 접속가능)
    <br> http://192.168.56.101:50070/
    <br http://192.168.56.101:8088/

    - 오류찾기
    <br> [hadoop@nn01 current]$ ls -al /opt/hadoop/current/logs/
    
    - set nu 하기
    <br> [영구적] vi /etc/virc
    ~~~shell
    set nu
    ~~~
    <br> [일시적] 파일 안 :set nu / 파일 밖 cat -n

    - su - root 하기
    <br> 1) [root@nn01 ~]# vi /etc/pam.d/su
    <br> 2) 10,11,12 라인에 vagrant 라고 보이는 3줄 주석처리
    <br> 3) [root@nn01 ~]# su - hadoop

    - 나갈 때 stop-all.sh  꼭!!!

* Hadoop으로 Java 파일 분산처리
    1. 이클립스에서 메이븐웹 생성 
    ~~~java
    package sample;
    import java.io.IOException;
    import java.util.StringTokenizer;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
    public class WordCount 
    {
        public static class MyMapper extends Mapper<LongWritable, Text, Text, LongWritable> 
        // 입력<key, value> : <Long, Text>
        // 출력<key, value> : <Text, Long>
        {
            // 출력 Key, value
            private Text word = new Text();
            private final static LongWritable one = new LongWritable(1);

            public void map(LongWritable Key, Text value, Context context) throws IOException, InterruptedException 
            {
                // 문자열로 자라준다.
                String line = value.toString();
                // 모든 이유로 잘라준다.
                StringTokenizer itr = new StringTokenizer(line, "\t\r\n\f |:;,.()<>");
                while(itr.hasMoreTokens()) 
                {
                    word.set(itr.nextToken());
                    context.write(word, one);    
                } // while-end
            } // mpa-end
        } // MyMapper-end
        public static class MyReducer extends Reducer<Text, LongWritable, Text, LongWritable> 
        // 입력<key, value> : <Long, Text>
        // 출력<key, value> : <Text, Long>
        {
            private LongWritable result = new LongWritable();
            public void reduce(Text key, Iterable<LongWritable> value, Context context) throws IOException, InterruptedException 
            {
                int sum = 0;
                for(LongWritable val : value) 
                {
                    sum += val.get();
                } // for-end
                result.set(sum); // java int형 -> hadoop LongWritable형으로 지정
                context.write(key, result); 
            } // reduce-end
            
        } // MyReducer-end
        public static void main(String[] args) throws Exception
        {
            Configuration conf = new Configuration();
            if(args.length != 2) 
            {
                System.out.println("사용: WordCount <input> <output>");
                System.exit(2);
            } // if-end
            Job job = Job.getInstance(conf, "WordCount");
            // 각 클래스 지정
            job.setJarByClass(WordCount.class);
            job.setMapperClass(MyMapper.class);
            job.setReducerClass(MyReducer.class);
            // 출력 key와 value 타입 지정
            job.setOutputKeyClass(Text.class);
            job.setOutputValueClass(LongWritable.class);
            // 입력포맷과 출력포맷 지정
            job.setInputFormatClass(TextInputFormat.class);
            job.setOutputFormatClass(TextOutputFormat.class);
            // 파일입력포맷과 파일출력포맷 지정
            FileInputFormat.addInputPath(job, new Path(args[0]));
            FileOutputFormat.setOutputPath(job, new Path(args[1]));
            // job을 실행
            // job이 다 실행하고 끝날때까지 기다리기
            job.waitForCompletion(true);
        } // main-end  
    } // wordcount-end
    ~~~
    2. Maven -> Update Project 후 Run As -> Maven install
    3. 생성된 .jar 파일을 WinSCP로 /home/hadoop 에서 /source 디렉터리 생성한 후 업로드
    4. 보낸 .jar 파일 확인 및 실행
    <br> 1) ls source
    <br> 2) mkdir data
    <br> 3) vi data/mydata.txt
    ~~~shell
    good monring
    i loves coffee
    hello world
    ~~~
    <br> 4) hdfs dfs -mkdir -p /input/data
    <br> 5) hdfs dfs -ls /input/data
    <br> 6) hdfs dfs -put /home/hadoop/data/mydata.txt /input/data
    <br> 7) hdfs dfs -ls /input/data
    <br> 8) yarn jar /home/hadoop/source/lab1.jar sample.WordCount /input/data/mydata.txt /output/wordcount
    <br> 9) hdfs dfs -ls /output/wordcount
    <br> 10) hdfs dfs -cat /output/wordcount/part-r-00000

## 정리
* core-site.xml
    - HDFS와 맵리듀스에서 공통적으로 사용할 환경 정보를 설정
    - core-site.xml에 설정값이 없을 경우 core-default.xml에 있는 기본값을 사용
* hdfs-site.xml
    - HDFS에서 사용할 환경 정보를 설정
    - hdfs-site.xml에 설정값이 없을 경우 hdfs-default.xml에 있는 기본값을 사용
* yarn-site.xml
    - Hadoop2에서 자원관리하는 정보를 설정
* mapred-site.xml
    - 맵리듀스에서 사용할 환경 정보를 설정
    - mapred-site.xml에 설정값이 없을 경우 mapred-default.xml에 있는 기본값을 사용
* masters
    - 보조네임노드를 실행할 서버를 설정
    - (*) 마스트노드(즉 네임노드)를 설정하는 것이 아님
* slaves
    - 데이터노드를 실행할 서버를 설정
* hadoop-env.sh(hadoop1)
    - 하둡을 실행하는 쉘 스크립트 파일에서 필요한 환경변수 설정
    - JDK 경로, 클래스 패스, 데몬 실행 옵션 등 환경 변수를 설정
* yarn-env.sh(hadoop2에서 추가)
* Hadoop 명령어
    1. 파일 목록보기 : ls, lsr
        - ls : 지정한 디렉토리에 있는 파일의 정보를 출력
        - lsr : 하위 디렉토리 정보까지 출력
    2. 파일용량파일 : du, dus
        - du : 지정한 디렉토리나 파일의 사용량을 확인 ( 출력결과 바이트 단위 )
        - dus : 전체 합계 용량 출력결과
    3. 파일내용보기 : cat, text
        - cat : 지정한 파일의 내용을 출력
        - text : cat은 텍스트파일만 출력하는데 text는 zip 파일 형태로 압축한 파일도 텍스트형태로 출력
    4. 디렉터리생성 : mkdir
    5. 파일복사
        - put / copyFromLocal : 로컬 파일 시스템의 파일 및 디렉토리를 HDFS의 경로로 복사
        - get / copyToLocal : HDFS에 저장된 데이타를 로컬 파일 시스템으로 복사
        - getmerge : 모든 파일의 내용을 하나로 합친 후, 로컬파일 시스템에 단 하나의 파일로 복사
        - cp : HDFS에서 디텍토리나 파일 복사
    6. 파일이동
        - mv : 디렉토리나 파일을 목적지 경로로 이동
        - moveFromLocal : put명령어와 동일한 로컬파일 시스템으로 복사된 후 소스 경로의 파일은 삭제
    7. 삭제
        - rm : 디렉토리나 파일 삭제, 디렉토리인 경우 반드시 비어 있어야 삭제
        - rmr : 디렉토리나 파일 삭제, 디렉토리인 경우 비어 있지 않아도 삭제
    8. 카운트 조회 
        - count : 지정한 경로에 대한 전체 디렉토리 개수, 전체 파일 개수, 전체 파일 크기 출력
    9. 권한변경
        - chmod : 지정한 경로에대한 권한 변경
        - chown : 지정한 파일과 디렉토리에 대한 소유권을 변경
        - chgrp : 지정한 파일과 디렉토리에 대한 소유권 그룹만 변경
        - -R : 하위 디렉토리의 정보도 모두 변경
    10. 통계정보조회 : stat
    11. 휴지통비우기 : expunge
    12. 0바이트 파일 생성 : touchz