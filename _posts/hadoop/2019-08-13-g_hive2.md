---
layout: post
title: hive2
category: hadoop
tags: [hadoop, hive]
comments: false
---

# [hive2]()

## 학습
* Hive 테이블 생성
    - 내부 테이블
    - 외부 테이블
* 파티션 테이블
    - insert
    - load
    - 외부테이블 파티션
    - 다중컬럼 파티션
* SQOOP 설치

## 실습
* hivedemo
    - 접속
~~~
1) syetemctl status mariadb.service로 상태 확인 후
2) active가 안되있을 시 syetemctl restart mariadb.service
3) active가 되있으면 syetemctl start mariadb.service
~~~
    
    - 백그라운드로 실행하기 
~~~
1) [hadoop@dn01 ~]$ hive --service metastore &
~~~

    - 백그라운드 실행 확인하기
~~~
1) [hadoop@dn01 ~]$ ps -ef | grep metastore
~~~

    - 하이브서버 백그라운드 실행하기
~~~
1) [hadoop@dn01 ~]$ hive --service HiveServer2 &
~~~

    - 하이브 실행(hive)
~~~
1) [hadoop@dn01 ~]$ hive
~~~

    - 하이브 실행(beeline)
~~~
1) [hadoop@dn01 ~]$ beeline
2) beeline> !connect jdbc:hive2://
3) Enter username for jdbc:hive2://: hive
4) Enter password for jdbc:hive2://: 1234
~~~

* u.item
    - 생성
~~~
1) hive (default)> show databases;
2) hive (default)> create database hivedemo;
3) hive (default)> use hivedemo;
~~~

    - 샘플 예문 확인
~~~
1) [hadoop@dn01 ~]$ hdfs dfs -ls -R /user/hadoop (입력된 데이터를 저장하는 곳)
2) [hadoop@dn01 ~]$ hdfs dfs -ls -R /user/hive/warehouse (hive가 기본적으로 저장하는 곳)
~~~

    - 테이블 만들기
~~~
CREATE TABLE movies (
movie_id INT,
movie_title STRING,
release_date STRING,
video_release_date STRING,
imdb_url STRING,
unknown INT,
action INT,
adventure INT,
animation INT,
children INT,
comedy INT,
crime INT,
documentary INT,
drama INT,
fantasy INT,
film_noir INT,
horror INT,
musical INT,
mystery INT,
romance INT,
sci_fi INT,
thriller INT,
war INT,
Western INT
)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;
~~~

    - 테이블 로드
~~~
1) hive (hivedemo)> load data inpath '/user/hadoop/movies/u.item/' into table movies;
~~~

* 테이블 삭제
    - truncate
~~~
1) hive (hivedemo)> truncate table movies; (삭제)
2) hive (hivedemo)> select * from movies; (확인)
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/movies (확인)
~~~

    - drop
~~~
1) hive (hivedemo)> load data local inpath '/home/hadoop/hive_data/ml-100k/u.item' into table movies; (재테이블로드)
2) [hadoop@dn01 ~]$ ll /home/hadoop/hive_data/ml-100k (파일이동되었나 확인)
3) hive (hivedemo)> drop table movies; (삭제)
4) hive (hivedemo)> select * from movies; (확인)
5) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/ (확인)
~~~

* u.user
    - 데이터 확인
~~~
1) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/userinfo
~~~

    - 테이블 생성
~~~
CREATE EXTERNAL TABLE users (
user_id INT,
age INT,
gender STRING,
occupation STRING,
zip_code STRING
)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;
~~~

    - 테이블 로드 및 확인
~~~
1) hive (hivedemo)> load data inpath '/user/hadoop/userinfo/u.user' into table users;
2) hive (hivedemo)> select * from users;
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/userinfo
4) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users
~~~

    - 테이블 drop 후 확인
~~~
1) hive (hivedemo)> drop table users;
2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users (외부테이블이라 파일이 안지워짐)
~~~

    - 테이블2 생성
~~~
CREATE EXTERNAL TABLE users (
user_id INT,
age INT,
gender STRING,
occupation STRING,
zip_code STRING
)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
LOCATION '/user/hadoop/hive_data/userinfo2';
~~~
    
    - 테이블 로드 및 확인
~~~
1) hive (hivedemo)> load data local inpath '/home/hadoop/hive_data/ml-100k/u.user' into table users;
2) hive (hivedemo)> select * from users;
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/userinfo
4) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users
~~~

    - 테이블 drop 후 확인
~~~
1) hive (hivedemo)> drop table users;
2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/users (안지워짐)
~~~

* partition
    - orders 테이블 생성
~~~
create table orders
(
id string,
customer_id string,
product_id string,
quantity int,
amount double,
zipcode char(5)
)
partitioned by (state char(2))
row format delimited fields terminated by ',';
1) hive (hivedemo)> desc formatted orders;
~~~
    
    - 데이터 파일 만들기 
~~~shell
1) [hadoop@dn01 hive_data]$ vi orders_CA.csv
########## orders_CA.csv ##########
o1,c1,p1,1,379.99,90210
o2,c2,p2,1,8.99,90002
o3,c3,p3,1,100,94305
o4,c4,p4,1,669,94304
########## orders_CA.csv ##########
2) [hadoop@dn01 hive_data]$ vi orders_CT.csv
########## orders_CT.csv ##########
o11,c100,p101,1,379.99,06010
o22,c200,p201,1,8.99,06911
########## orders_CT.csv ##########
~~~

    - 테이블 로드 및 확인
~~~
1) hive (hivedemo)> load data local inpath '/home/hadoop/hive_data/orders_CA.csv' into table orders partition(state='CA');
2) hive (hivedemo)> select * from orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/orders (파티션은 서브디렉터리로 만들어짐)
~~~

    - 테이블 로드 및 확인2
~~~
1) hive (hivedemo)> load data local inpath '/home/hadoop/hive_data/orders_CT.csv' into table orders partition(state='CT');
2) hive (hivedemo)> select * from orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/orders
~~~

    - all_orders 테이블 생성
~~~
create table all_orders
(
id string,
customer_id string,
product_id string,
quantity int,
amount double,
postalcode string
)
partitioned by (country string, state string)
row format delimited fields terminated by ',';
~~~

    - 테이블 로드 및 확인
~~~
1) hive (hivedemo)> load data local inpath '/home/hadoop/hive_data/orders_CA.csv' into table all_orders partition(country='US', state='CA');
2) hive (hivedemo)> select * from all_orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/all_orders/county=US (서브디렉터리가 2개 생성)
~~~

    - 테이블 로드 및 확인2
~~~
1) hive (hivedemo)> load data local inpath '/home/hadoop/hive_data/orders_CT.csv' into table all_orders partition(country='KR', state='CT');
2) hive (hivedemo)> select * from all_orders;
3) [hadoop@dn01 hive_data]$ hdfs dfs -ls /user/hive/warehouse/hivedemo.db/all_orders/county=KR
~~~

* sqoop
    - 설치
~~~
1) [root@dn01 ~]# cd /tmp
2) [root@dn01 tmp]# wget http://mirror.apache-kr.org/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
3) [root@dn01 tmp]# tar -xvzpf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
4) [root@dn01 tmp]# mkdir -p /opt/sqoop/1.4.7
5) [root@dn01 tmp]# mv sqoop-1.4.7.bin__hadoop-2.6.0/* /opt/sqoop/1.4.7/
6) [root@dn01 tmp]# ln -s /opt/sqoop/1.4.7 /opt/sqoop/current
~~~
    
    - mysql
~~~
1) [root@dn01 tmp]# tar -xvzpf mysql-connector-java-5.1.38.tar.gz
2) [root@dn01 tmp]# cd mysql-connector-java-5.1.38/
3) [root@dn01 mysql-connector-java-5.1.38]# cp mysql-connector-java-5.1.38-bin.jar /opt/sqoop/current/lib
4) [root@dn01 tmp]# chown hadoop:hadoop /opt/sqoop/current/lib/mysql-connector-java-5.1.38-bin.jar
~~~

    - 환경변수
~~~shell
1) [hadoop@dn01 ~]$ vi ~/.bash_profile
#### SQOOP 1.4.7 ##############
export SQOOP_HOME=/opt/sqoop/current
export SQOOP_PATH=$PATH:$SQOOP_HOME/bin
#### SQOOP 1.4.7 ##############
2) [hadoop@dn01 ~]$ source ~/.bash_profile
3) [hadoop@dn01 ~]$ cd $SQOOP_HOME
4) [hadoop@dn01 current]$ pwd
5) [hadoop@dn01 current]$ cd $SQOOP_HOME/conf
6) [hadoop@dn01 conf]$ mv sqoop-env-template.sh sqoop-env.s
7) [hadoop@dn01 conf]$ vi sqoop-env.sh
########## sqoop-env.sh ###########
#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=/opt/hadoop/current
#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/opt/hadoop/current
#Set the path to where bin/hive is available
export HIVE_HOME=/opt/hive/current
########## sqoop-env.sh ###########
8) [hadoop@dn01 conf]$ cd $SQOOP_HOME
9) [hadoop@dn01 current]$ cp sqoop-1.4.7.jar $HADOOP_HOME/share/hadoop/tools/lib
10) [hadoop@dn01 current]$ ll $HADOOP_HOME/share/hadoop/tools/lib
11) [hadoop@dn01 current]$ sqoop-version (설치확인)
~~~

## 정리
* 테이블 삭제
    - delete from 테이블명 : 데이터만 삭제후에 메모리 자리 남김(hive는 지원안함)
    - drop table 테이블명 : 데이터 + 구조 삭제
    - truncate 테이블명 : 데이터 삭제후에 메모리 반환
    - hdfs는 메모리가 아니라 파일로 저장하는데 truncate 결과 확인 : 파일삭제
* CREATE EXTERNAL TABLE
    - 사용자가 실수로 테이블을 DROP 했더라도 데이터가 보존됨(메타데이터는 지워짐)
* hive와 hdfs : 테이블 = 디렉터리, 파티션 = 서브디렉터리, 데이터 = 파일  