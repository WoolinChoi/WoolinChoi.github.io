---
layout: post
title: pig
category: hadoop
tags: [hadoop, pig]
comments: false
---

# [pig]()

## 학습
* pig 이해

## 실습
* 설정
    - mapred-site.xml 설정
~~~shell
1) [hadoop@nn01 ~]$ vi $HADOOP_HOME/etc/hadoop/mapred-site.xml
########## mapred-site.xml ##########
# <property>
# <name>mapreduce.jobhistory.address</name>
# <value>nn01:10020</value>
# </property>
########## mapred-site.xml ##########
2) [hadoop@nn01 ~]$ cd $HADOOP_HOME/etc/hadoop
3) [hadoop@nn01 hadoop]$ scp mapred-site.xml hadoop@dn01:/opt/hadoop/current/etc/hadoop
4) [hadoop@nn01 hadoop]$ scp mapred-site.xml hadoop@dn02:/opt/hadoop/current/etc/hadoop
~~~

    - dn01, dn02의 mapred-site.xml 확인하기
~~~
1) [hadoop@dn01 ~]$ vi $HADOOP_HOME/etc/hadoop/mapred-site.xml
2) [hadoop@dn02 ~]$ vi $HADOOP_HOME/etc/hadoop/mapred-site.xml
~~~

    - stop, start 하기
~~~
1) [hadoop@nn01 hadoop]$ stop-all.sh
2) [hadoop@nn01 hadoop]$ start-all.sh
3) [hadoop@nn01 hadoop]$ jps
~~~

    - historyserve 하기(mapreduce 가능하기위해)
~~~shell
1) [hadoop@nn01 hadoop]$ cd $HADOOP_HOME/sbin
2) [hadoop@nn01 sbin]$ mr-jobhistory-daemon.sh start historyserver
3) [hadoop@nn01 sbin]$ jps
########## jps ##########
29812 NameNode
30166 ResourceManager
30473 JobHistoryServer
30010 SecondaryNameNode
30510 Jps
########## jps ##########
~~~

    - mapreduce 접속
~~~
1) [hadoop@dn01 ~]$ pig -x mapreduce
2) grunt> groceries = load 'hdfs://nn01:9000/input/pig_data/groceries.csv' using PigStorage(',');
3) grunt> dump groceries
4) grunt> store groceries into '/output/pig_data/store_dir' using PigStorage();
~~~

    - output 확인
~~~
1) [hadoop@dn02 ~]$ ssh dn01
2) [hadoop@dn01 ~]$ hdfs dfs -ls /output/pig_data/store_dir
3) [hadoop@dn01 ~]$ hdfs dfs -cat /output/pig_data/store_dir/part-m-00000
~~~

* 스키마 구조
    - groceries.csv 처리
~~~
1) [hadoop@dn01 ~]$ pig -x local
2) grunt> groceries = load 'file:///home/hadoop/pig_data/groceries.csv' using PigStorage(',')
as (
order_id:chararray,
location:chararray,
product:chararray,
day:datetime,
revenue:double);
3) grunt> describe groceries;
4) grunt> products = foreach groceries generate location, product, day;
5) grunt> dump products
6) grunt> store products into 'file:///home/hadoop/pig_data/output/store_dir' using PigStorage();
~~~

    - 처리 확인
~~~
1) [hadoop@dn02 ~]$ ssh dn01
2) [hadoop@dn01 ~]$ ls /home/hadoop/pig_data/output/store_dir
3) [hadoop@dn01 ~]$ cat /home/hadoop/pig_data/output/store_dir
~~~

    - limit 확인
~~~
1) grunt> groceries_sub = limit groceries 5;
2) grunt> dump groceries_sub
~~~

* 복합 자료형
    - student_subjects.txt 처리
~~~
1) grunt> student_subjects = load 'file:///home/hadoop/pig_data/data/student_subjects.txt' 
as (
student_id:chararray,
name:chararray,
grade:int,
subject1:chararray,
subject2:chararray,
subject3:chararray);
2) dump student_subjects
~~~

    - bag 처리
~~~
1) grunt> student_subjects2 = foreach student_subjects generate name, TOBAG(subject1, subject2, subject3) as subject;
2) grunt> student_subject3 = foreach student_subjects2 generate subject;
3) dump student_subjects3
~~~

    - student_marks.txt
~~~
1) grunt> student_marks = load 'file:///home/hadoop/pig_data/data/student_marks.txt'
as (
student_id:chararray,
name:chararray,
grade:int,
marks:map[int]);
2) grunt> student_marks2 = foreach student_marks generate name, marks#'Math';
3) grunt> dump student_marks2
~~~

    - map 처리
~~~
1) grunt> student_marks3 = foreach student_marks generate student_id, TOMAP(name, grade);
2) grunt> dump student_marks3
~~~

    - students.txt 처리
~~~
1) grunt> students = load 'file:///home/hadoop/pig_data/data/students.txt'
as (
stu_id:chararray,
name:chararray,
grade:int,
contact:tuple(addr:chararray, tel:chararray));
2) grunt> dump student
~~~

    - tuple 처리
~~~
1) grunt> students2 = foreach students generate name, contact.tel;
2) grunt> dump students2
~~~

    - flatten 처리(tuple을 풀어줌)
~~~
1) grunt> students3 = foreach student generate stu_id, name, grade, flatten(contact);
2) grunt> dump students3
~~~

    - wordcount 처리
~~~
1) grunt> A = load 'file:///opt/hadoop/current/README.txt';
2) grunt> dump A
3) grunt> B = foreach A generate flatten(TOKENIZE((chararray)$0)) as word;
4) grunt> dump B
5) grunt> C = group B by word;
6) grunt> dump C
7) grunt> D = foreach C generate group as word, COUNT($1) as count;
8) grunt> dump D 
~~~

* Function
    - orders.json
~~~
1) grunt> orders = load 'file:///home/hadoop/pig_data/data/orders.json' using JsonLoader('
order_id:chararray, 
username:chararray, 
product:chararray, 
quantity:int, 
amount:double, 
order_date:chararray, 
zipcode:chararray');
2) grunt> dump orders
3) grunt> orders2 = foreach orders generate username, product, quantity*amount as total;
4) grunt> dump orders2
~~~

    - ROUND() 함수
~~~
1) grunt> orders2 = foreach orders generate ROUND(quantity*amount) as total
~~~

    - SUBSTING() 함수
~~~
1) grunt> orders2 = foreach orders generate SUBSTRING(order_id, 1, 3);
~~~

    - REPLACE 함수
~~~
1) grunt> orders2 = foreach orders generate REPLACE(order_id,'o', 'order');
~~~

    - 주문번호로 정렬
~~~
1) grunt> result = order orders by order_id DESC;
~~~

    - 총가격 순으로 정렬하고 상위 5개 출력
~~~
1) grunt> result = foreach orders generate product, username, ROUND(quantity*amount) as total;
2) grunt> result2 = order result by total DESC;
3) grunt> result3 = limit result2 5; 
~~~

    - 주문 날짜의 월단위로 정렬(getMonth() 함수)
~~~
1) grunt> result = foreach orders generate product, username, ToDate(order_date, 'MM-dd-yyyy') as date;
2) grunt> result2 = foreach result generate product, username, GetMonth(date) as month;
3) grunt> result3 = order result2 by month;
4) grunt> dump result3
~~~

* Filter
    - 수량이 없는 튜플 제외하기
~~~
1) grunt> result = filter orders by quantity > 0;
~~~

    - 주문번호가 'o4'이상인 튜플들 추출
~~~
1) grunt> result2 = filter result by order_id >= 'o4';
~~~

    - 상품명이 'tv'로 끝나는 튜플들 추출
~~~
1) grunt> result = filter orders by product matches '.*tv';
~~~

    - 상품명이 'speakers'가 포함된 튜플들 추출
~~~
1) grunt> result = filter orders by product matches '.*speakers';
~~~

 * Split
    - 개수가 1인 bag와 1초과인 bag으로 나누기
~~~
1) grunt> split orders into result_1 if(quantity == 1), result_more if(quantity > 1);
2) grunt> dump result_1
3) grunt> dump result_more
~~~

    - 사용자명이 있는 bag와 사용자명이 없는 bag으로 나누기
~~~
1) grunt> split orders into result_notnull if(username is not null), result_null if(username is null);
2) grunt> dump result_notnull;
3) grunt> dump result_null;
~~~

* Distinct(중복제거)
    - orders_duplicates.json
~~~
1) grunt> rep_orders = load 'file:///home/hadoop/pig_data/data/orders_duplicates.json' using JsonLoader('
order_id: chararray,
username: chararray,
product: chararray,
quantity: int,
amount: double,
order_date: chararray,
zipcode: chararray');
2) grunt> dump rep_orders;
3) grunt> result = distinct rep_orders;
4) grunt> dump result;
~~~

* Join
    - emp.csv 파일 로드하여 변수 emp 저장
~~~
1) grunt> emp = load 'file:///home/hadoop/pig_data/data/emp/emp.csv' using PigStorage(',');
~~~

    - dept.csv 파일 로드하여 변수 dept 저장
~~~
1) grunt> dept = load 'file:///home/hadoop/pig_data/data/emp/dept.csv' using PigStorage(',');
~~~

    - 두 bag을 조인하기
~~~
1) grunt> emp_join = JOIN emp BY $7, dept BY $2;
~~~

    - 조인 bag에서 월급이 2000 초과한 내역을 월급순으로 정렬하여 출력(filter 이용)
~~~
1) grunt> emp_join = JOIN emp BY $7, dept BY $2;
2) grunt> result = FILTER emp_join BY $5 > 2000;
3) grunt> result2 = ORDER result BY $5 DESC;
4) grunt> dump result2;
~~~

## 정리
* Pig 명령어
    - DUMP : 결과를 화면에 출력 가능
    - STORE : 결과를 파일로 저장 가능
    - DESCRIBE : 데이터 구조에 대한 정보를 보여줌
* FOREACH
    - 필요한 컬럼들만 결과로 만들기
* DISTINCT
    - Bag안에 중복 튜플들을 삭제
* LIMI
    - 출력되는 튜플의 개수 제한
* 스키마
    - 스카마가 없으면 describe 구조 없음, 컬럼을 $숫자로 추출
    - 스키마가 있으면 describe 구조 확인, 컬럼명으로 추출
* 복합자료형
    - bag
    - map
    - tuple