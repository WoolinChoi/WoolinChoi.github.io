---
layout: post
title: hive
category: hadoop
tags: [hadoop, hive]
comments: false
---

# [hive]()

## 학습
* MariaDB 설치
* hive 설치 및 이해

## 실습
* Mariadb
    - 설치
~~~
1) [root@dn01 ~]# yum install mariadb-server mariadb
2) [root@dn01 ~]# cd /tmp
3) [root@dn01 tmp]# systemctl enable mariadb.service
4) [root@dn01 tmp]# systemctl start mariadb.service
5) [root@dn01 tmp]# mysql_secure_installation
6) Enter current password for root (enter for none): enter
7) Set root password? [Y/n] n
8) New password: 1234
9) Re-enter new password: 1234
8) Remove anonymous users? [Y/n] n
9) Disallow root login remotely? [Y/n] n
10) Remove test database and access to it? [Y/n] n
11) Reload privilege tables now? [Y/n]
~~~
    
    - 설정
~~~shell
1) [root@dn01 tmp]# vi /etc/my.cnf
########## my.cnf ##########
bind-address=192.168.58.102
########## my.cnf ##########
2) [root@dn01 tmp]# systemctl restart mariadb.service
~~~

    - 접속
~~~
1) [root@dn01 tmp]# mysql -u root -p
2) Enter password: 1234
3) MariaDB [(none)]> show databases;
~~~

    - 권한 부여
~~~
1) MariaDB [(none)]> grant all privileges on *.* to hive@'%' identified by "hive" with grant option;
2) MariaDB [(none)]> flush privileges;
3) MariaDB [(none)]> grant all privileges on *.* to hive@'dn01' identified by "hive" with grant option;
4) MariaDB [(none)]> flush privileges;
~~~

    - 권한 부여 확인
~~~
1) MariaDB [(none)]> use mysql;
2) MariaDB [mysql]> select user, host from user;
~~~

* Hive
    - 설치
~~~
1) [root@dn01 ~]# cd /tmp
2) [root@dn01 tmp]# wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz
3) [root@dn01 tmp]# tar xzvf apache-hive-2.3.5-bin.tar.gz
4) [root@dn01 tmp]# mkdir -p /opt/hive/2.3.5
5) [root@dn01 tmp]# mv apache-hive-2.3.5-bin/* /opt/hive/2.3.5/
6) [root@dn01 tmp]# ln -s /opt/hive/2.3.5 /opt/hive/current
~~~

    - 환경 변수 추가
~~~shell
1) [hadoop@dn01 ~]$ vi ~/.bash_profile
#### HIVE 2.3.5 #######################
export HIVE_HOME=/opt/hive/current
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=.:${JAVA_HOME}/lib:${JREHOME}/lib:/opt/hive/current/lib
#### HIVE 2.3.5 #######################
2) [hadoop@dn01 ~]$ source .bash_profile
~~~

    - 설정 파일 복사
~~~
1) [hadoop@dn01 ~]$ cp /opt/hive/current/conf/hive-env.sh.template /opt/hive/current/conf/hive-env.sh
2) [hadoop@dn01 ~]$ cp /opt/hive/current/conf/hive-default.xml.template /opt/hive/current/conf/hive-site.xml
~~~

    - 설정 파일 수정
~~~shell
1) vi /opt/hive/current/conf/hive-env.sh
######### hive-env.sh ##########
HADOOP_HOME=/opt/hadoop/current
######### hive-env.sh ##########
2) [hadoop@dn01 ~]$ vi /opt/hive/current/conf/hive-site.xml (value만 바꾸기)
########## hive-site.xml ##########
# <property>
# <name>javax.jdo.option.ConnectionURL</name>
# <value>jdbc:mysql://192.168.56.102:3306/hive?createDatabaseIfNotExist=true</value>
# <description>JDBC connect string for a JDBC metastore</description>
# </property>
# <property>
# <name>javax.jdo.option.ConnectionDriverName</name>
# <value>com.mysql.jdbc.Driver</value>
# <description>Driver class name for a JDBC metastore</description>
# </property>
# <property>
# <name>javax.jdo.option.ConnectionUserName</name>
# <value>hive</value>
# <description>username to use against metastore database</description>
# </property>
# <property>
# <name>javax.jdo.option.ConnectionPassword</name>
# <value>hive</value>
# <description>password to use against metastore database</description>
# </property>
# <property> 
# <name>hive.exec.local.scratchdir</name>
# <value>/home/hadoop/iotmp</value>
# <description>Local scratch space for Hive jobs</description>
# </property>
# <property>
# <name>hive.downloaded.resources.dir</name>
# <value>/home/hadoop/iotmp</value>
# <description>Temporary local directory for added resources in the remote file system.</description>
# </property>
# <property>
# <name>hive.cli.print.current.db</name>
# <value>true</value>
# <description>Whether to include the current database in the Hive prompt.</description>
# </property>
########## hive-site.xml ##########
~~~

    - 관련 디렉토리 생성 및 권한 변경
~~~
1) [hadoop@dn01 ~]$ mkdir -p /home/hadoop/iotmp
2) [hadoop@dn01 ~]$ chmod -R 775 /home/hadoop/iotmp
~~~

    - MYSQL Connector 다운로드 및 hive lib로 복사
~~~
1) [root@dn01 ~]# cd /tmp
2) [root@dn01 tmp]# wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.38.tar.gz
3) [root@dn01 tmp]# tar xzvf mysql-connector-java-5.1.38.tar.gz
4) [root@dn01 tmp]# cd mysql-connector-java-5.1.38
5) [root@dn01 mysql-connector-java-5.1.38]# mv mysql-connector-java-5.1.38-bin.jar /opt/hive/current/lib/
~~~

    - 기본 디렉터리 생성 및 권한 추가
~~~
1) [hadoop@dn01 ~]$ hdfs dfs -mkdir /tmp
2) [hadoop@dn01 ~]$ hdfs dfs -mkdir -p /user/hive/warehouse
3) [hadoop@dn01 ~]$ hdfs dfs -chmod -R 777 /tmp
4) [hadoop@dn01 ~]$ hdfs dfs -chmod -R 777 /user/hive/warehouse
~~~

    - mysql 기본 스키마 생성
~~~
1) [hadoop@dn01 ~]$ schematool -initSchema -dbType mysql
error) 기존에 같은 이름의 데이타베이스가 있으니깐 show databases에서 drop database hive; 제거
~~~

    - 접속
~~~
1) [hadoop@dn01 ~]$ hive
check) http://192.168.56.101:50070에서 메뉴>Utitlies>Browser Directory>/user/hive/warehouse에서 앞으로 확인
~~~

    - beeline 접속하기 위한 추가 작업
~~~
1) [hadoop@nn01 ~]$ cd $HADOOP_HOME/etc/hadoop
2) [hadoop@nn01 hadoop]$ vi core-site.xml
3) [hadoop@nn01 hadoop]$ scp core-site.xml hadoop@dn01:/opt/hadoop/current/etc/hadoop
4) [hadoop@nn01 hadoop]$ scp core-site.xml hadoop@dn02:/opt/hadoop/current/etc/hadoop
5) [hadoop@dn01 ~]$ hdfs dfs -chmod -R 777 /tmp
6) [hadoop@dn01 ~]$ hdfs dfs -chmod -R 777 /user/hive/warehouse
~~~

* Hive 실행
    - 상태확인
~~~
1) [hadoop@dn01 ~]$ systemctl status mariadb.service
~~~

    - 시작
~~~
1) [hadoop@dn01 ~]$ systemctl start mariadb.service
~~~

    - 메타스토어 실행 및 확인(백그라운드 실행)
~~~
1) [hadoop@dn01 ~]$ hive --service metastore &
2) [hadoop@dn01 ~]$ ps -ef | grep metastore
~~~

    - 하이브서버 구동(백그라운드 실행)
~~~
1) [hadoop@dn01 ~]$ hive --service hiveserver2 &
~~~

* CLI : hive
    - 접속 및 sample 데이터베이스 생성
~~~
1) [hadoop@dn01 ~]$ hive
2) hive (default)> show databases;
3) hive (default)> create database sample1;
4) hive (default)> show databases;
5) hive (default)> use sample1;
6) hive (sample1)> create table employees( name String );
7) hive (sample1)> insert into employees(name) values('KIM');
~~~

    - 확인(테이블은 디렉터리, 값은 파일로 저장해준다.)
~~~
1) [hadoop@dn02 ~]$ ssh dn01
2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse
3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/sample1.db
4) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/sample1.db/employees
5) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/sample1.db/employees/000000_0
6) hive (sample1)> !hdfs dfs -cat /user/hive/warehouse/sample1.db/employees/000000_0;
7) hive (sample1)> !hdfs dfs -ls /user/hive/warehouse/sample1.db/employees;
8) hive (sample1)> exit; (종료)
~~~

* CLI : beeline
    - 접속
~~~
1) [hadoop@dn01 ~]$ beeline
2) beeline> !connect jdbc:hive2://
3) Enter username for jdbc:hive2://: hive
4) Enter password for jdbc:hive2://: hive
~~~

    - sample데이터베이스 추가 및 확인
~~~
1) 0: jdbc:hive2://> show databases;
2) 0: jdbc:hive2://> use sample1;
3) 0: jdbc:hive2://> show tables;
4) 0: jdbc:hive2://> insert into employees(name) values('PARK');
5) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/sample1.db/employees/000000_0_copy_1
6) 0: jdbc:hive2://> select * from employees;
~~~

* 연습
    - kdatademo데이터베이스 생성
~~~
1) 0: jdbc:hive2://> create database kdatademo;
2) 0: jdbc:hive2://> show databases;
3) 0: jdbc:hive2://> use kdatademo;
4) 테이블 생성
create table customers(id bigint, name string, address string);
5) 0: jdbc:hive2://> show tables;
6) 0: jdbc:hive2://> desc customers;
7) 입력
insert into customers values
(1111, 'kim', 'WA'),
(2222, 'park', 'WA'),
(3333, 'lee', 'WA'),
(4444, 'meng', 'CA'),
(5555, 'bae', 'NJ'),
(6666, 'jeon', 'NY');
~~~

    - 확인
~~~
1) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/kdatademo.db/customers
2) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/kdatademo.db/customers/000000_0
~~~

    - 생성
~~~
1) 테이블 생성
create table if not exists orders(
id bigint,
product_id string,
customer_id bigint,
quantity int,
amount double);
2) 입력
insert into orders values
(111111, "phone", 1111, 3, 1200),
(222222, "camera", 1111, 1, 5200),
(100003, "notebook", 1111, 1, 10),
(100004, "bag", 2222, 1, 4500),
(100005, "t-shirt", 4444, 30, 50);
~~~

    - 주소가 "WA"인 고객 검색
~~~
1) 0: jdbc:hive2://> select * from customers where address="WA";
~~~

    - 주소가 "WA"이면서 id가 2222 보다 큰 고객 명단 출력
~~~
1) 0: jdbc:hive2://> select * from customers where address="WA" AND id>2222;
~~~
    
    - 주소로 정렬하여 고객명과 주소 출력
~~~
1) 0: jdbc:hive2://> select name, address from customers order by address;
~~~

    - 고객 명단 수 검색
~~~
1) 0: jdbc:hive2://> select count(*) from customers;
~~~

    - 첫번째 고객명단 출력(mysql 명령어 이용)
~~~
1) 0: jdbc:hive2://> select * from customers limit 1;
~~~

    - 주소별 인원수 검색
~~~
1) 0: jdbc:hive2://> select address, count(*) cnt from customers group by address;
~~~

    - 고객아이디, 고객명과 고객이 주문한 상품명 출력
~~~
1) 0: jdbc:hive2://> select c.id, c.name, o.product_id from customers c left outer join orders o on (c.id=o.customer_id);
~~~

* mobilephones
    - 생성
~~~shell
1) [hadoop@dn01 ~]$ mkdir /home/hadoop/hive_data
2) [hadoop@dn01 ~]$ vi /home/hadoop/hive_data/mobilephones.csv
########## mobilephones.csv ##########
Samsungg7,Samsung G7,250,red#white#blue,5.5,camera:true#dualsim:false
Iphoneplus7,Iphone plus 7,400,gold#white,4.5,autofocus:true
########## mobilephones.csv ##########
3) 테이블 생성
create table mobilephones(
id string,
title string,
cost double,
colors array<string>,
screen_size array<float>,
features map<string, Boolean>)
row format delimited fields terminated by ','
collection items terminated by '#'
map keys terminated by ':';
~~~

    - 확인
~~~
1) 0: jdbc:hive2://> load data local inpath '/home/hadoop/hive_data/mobilephones.csv' into table mobilephones;
2) 0: jdbc:hive2://> select * from mobilephones;
3) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/kdatademo.db/mobilephones/mobilephones.csv
~~~

    - 제품아이디와 첫번째 색상 출력
~~~
1) 0: jdbc:hive2://> select id, colors[0] from mobilephones;
~~~

    - 제품명과 카메라특성 출력
~~~
1) 0: jdbc:hive2://> select id, features['camera'] from mobilephones;
~~~

* mobilephones2
    - 생성
~~~shell
1) [hadoop@dn01 ~]$ cp ~/hive_data/mobilephones.csv ~/hive_data/mobilephones2.csv
########### mobilephones2.csv ##########
Samsungg7,Samsung G7,250,red#white#blue,5.5,camera:true#dualsim:false,24 hours#2MP
Iphoneplus7,Iphone plus 7,400,gold#white,4.5,autofocus:true,2 hours#2MP
########### mobilephones2.csv ##########
2) 테이블 생성
create table mobilephones2(
id string,
title string,
cost double,
colors array<string>,
screen_size array<float>,
features map<string, Boolean>,
infomation struct<battery:string,camera:string>)
row format delimited fields terminated by ','
collection items terminated by '#'
map keys terminated by ':';
~~~

    - 확인
~~~
1) 0: jdbc:hive2://> load data local inpath '/home/hadoop/hive_data/mobilephones2.csv' into table mobilephones2;
2) 0: jdbc:hive2://> select * from mobilephones2;
3) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/kdatademo.db/mobilephones/mobilephones2.csv
~~~

    - 제품아이디, 첫번째 색상, 카메라 특성, 배터리 정보 출력
~~~
1) 0: jdbc:hive2://> select id, colors[0], features['camera'], infomation.battery from mobilephones2;
~~~    

* grouplens
    - 설치
~~~
1) [root@dn01 ~]$ yum install unzip
2) [hadoop@dn01 ~]$ cd ~/hive_data
3) [hadoop@dn01 hive_data]$ wget http://www.grouplens.org/system/files/ml-100k.zip
4) [hadoop@dn01 hive_data]$ unzip ml-100k.zip
5) [hadoop@dn01 hive_data]$ cd ml-100k
6) [hadoop@dn01 ml-100k]$ hdfs dfs -mkdir /user/hadoop/movies
7) [hadoop@dn01 ml-100k]$ hdfs dfs -mkdir /user/hadoop/userinfo
8) [hadoop@dn01 ml-100k]$ hdfs dfs -put /home/hadoop/hive_data/ml-100k/u.item /user/hadoop/movies
9) [hadoop@dn01 ml-100k]$ hdfs dfs -put /home/hadoop/hive_data/ml-100k/u.user /user/hadoop/userinfo
~~~

## 정리
* Hive
    - 하둡에서 동작하는 데이터 웨어하우스 인프라 구조로서 데이터 요약, 질의 및 분석 기능을 제공