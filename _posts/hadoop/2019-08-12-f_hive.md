---
layout: post
title: hive
category: hadoop
tags: [hadoop, hive]
comments: false
---

# [hive]()

## 학습
* MariaDB 설치
* hive 설치 및 이해

## 실습
* Mariadb
    - 설치
    <br> 1) yum install mariadb-server mariadb
    <br> 2) [root@dn01 ~]# cd /tmp
    <br> 3) [root@dn01 tmp]# systemctl enable mariadb.service
    <br> 4) [root@dn01 tmp]# systemctl start mariadb.service
    <br> 5) [root@dn01 tmp]# mysql_secure_installation
    <br> 6) Enter current password for root (enter for none): enter
    <br> 7) Set root password? [Y/n] n
    <br> 8) New password: 1234
    <br> 9) Re-enter new password: 1234
    <br> 8) Remove anonymous users? [Y/n] n
    <br> 9) Disallow root login remotely? [Y/n] n
    <br> 10) Remove test database and access to it? [Y/n] n
    <br> 11) Reload privilege tables now? [Y/n]
    
    - 설정
    <br> 1) [root@dn01 tmp]# vi /etc/my.cnf
    ~~~shell
    bind-address=192.168.58.102
    ~~~
    <br> 2) [root@dn01 tmp]# systemctl restart mariadb.service

    - 접속
    <br> 1) [root@dn01 tmp]# mysql -u root -p
    <br> 2) Enter password: 1234
    <br> 3) MariaDB [(none)]> show databases;
    
    - 권한 부여
    <br> 1) MariaDB [(none)]> grant all privileges on *.* to hive@'%' identified by "hive" with grant option;
    <br> 2) MariaDB [(none)]> flush privileges;
    <br> 3) MariaDB [(none)]> grant all privileges on *.* to hive@'dn01' identified by "hive" with grant option;
    <br> 4) MariaDB [(none)]> flush privileges;

    - 권한 부여 확인
    <br> 1) MariaDB [(none)]> use mysql;
    <br> 2) MariaDB [mysql]> select user, host from user;

* Hive
    - 설치
    <br> 1) [hadoop@dn01 ~]# cd /tmp
    <br> 2) [hadoop@dn01 tmp]# wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz
    <br> 3) [hadoop@dn01 tmp]# tar xzvf apache-hive-2.3.5-bin.tar.gz
    <br> 4) [hadoop@dn01 tmp]# mkdir -p /opt/hive/2.3.5
    <br> 5) [hadoop@dn01 tmp]# mv apache-hive-2.3.5-bin/* /opt/hive/2.3.5/
    <br> 6) [hadoop@dn01 tmp]# ln -s /opt/hive/2.3.5 /opt/hive/current

    - 환경 변수 추가
    <br> 1) [hadoop@dn01 ~]# vi ~/.bash_profile
    ~~~shell
    #### HIVE 2.3.5 #######################
    export HIVE_HOME=/opt/hive/current
    export PATH=$PATH:$HIVE_HOME/bin
    export CLASSPATH=.:${JAVA_HOME}/lib:${JREHOME}/lib:/opt/hive/current/lib
    #### HIVE 2.3.5 #######################
    ~~~
    <br> 2) [hadoop@dn01 ~]# source .bash_profile

    - 설정 파일 복사
    <br> 1) [hadoop@dn01 ~]# cp /opt/hive/current/conf/hive-env.sh.template /opt/hive/current/conf/hive-env.sh
    <br> 2) [hadoop@dn01 ~]# cp /opt/hive/current/conf/hive-default.xml.template /opt/hive/current/conf/hive-site.xml

    - 설정 파일 수정
    <br> 1) [hadoop@dn01 ~]# vi /opt/hive/current/conf/hive-env.sh
    ~~~shell
    HADOOP_HOME=/opt/hadoop/current
    ~~~
    <br> 2) [hadoop@dn01 ~]# vi /opt/hive/current/conf/hive-site.xml (value만 바꾸기)
    ~~~shell
    <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://192.168.56.102:3306/hive?createDatabaseIfNotExist=true</value>
    <description>JDBC connect string for a JDBC metastore</description>
    </property>
    <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
    </property>
    <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>username to use against metastore database</description>
    </property>
    <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
    <description>password to use against metastore database</description>
    </property>
    <property> 
    <name>hive.exec.local.scratchdir</name>
    <value>/home/hadoop/iotmp</value>
    <description>Local scratch space for Hive jobs</description>
    </property>
    <property>
    <name>hive.downloaded.resources.dir</name>
    <value>/home/hadoop/iotmp</value>
    <description>Temporary local directory for added resources in the remote file system.</description>
    </property>
    <property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
    <description>Whether to include the current database in the Hive prompt.</description>
    </property>
    ~~~

    - 관련 디렉토리 생성 및 권한 변경
    <br> 1) [hadoop@dn01 ~]# mkdir -p /home/hadoop/iotmp
    <br> 2) [hadoop@dn01 ~]# chmod -R 775 /home/hadoop/iotmp/

    - MYSQL Connector 다운로드 및 hive lib로 복사
    <br> 1) [hadoop@dn01 ~]# cd /tmp
    <br> 2) [hadoop@dn01 tmp]# wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.38.tar.gz
    <br> 3) [hadoop@dn01 tmp]# tar xzvf mysql-connector-java-5.1.38.tar.gz
    <br> 4) [hadoop@dn01 tmp]# cd mysql-connector-java-5.1.38
    <br> 5) [hadoop@dn01 mysql-connector-java-5.1.38]# mv mysql-connector-java-5.1.38-bin.jar /opt/hive/current/lib/

    - 기본 디렉터리 생성 및 권한 추가
    <br> 1) [hadoop@dn01 ~]# hdfs dfs -mkdir /tmp
    <br> 2) [hadoop@dn01 ~]# hdfs dfs -mkdir -p /user/hive/warehouse
    <br> 3) [hadoop@dn01 ~]# hdfs dfs -chmod -R 777 /tmp
    <br> 4) [hadoop@dn01 ~]# hdfs dfs -chmod -R 777 /user/hive/warehouse

    - mysql 기본 스키마 생성
    <br> 1) [hadoop@dn01 ~]# schematool -initSchema -dbType mysql
    <br> error) 기존에 같은 이름의 데이타베이스가 있으니깐 show databases에서 drop database hive; 제거

    - 접속
    <br> 1) [hadoop@dn01 ~]# hive
    <br> check) http://192.168.56.101:50070에서 메뉴>Utitlies>Browser Directory>/user/hive/warehouse에서 앞으로 확인

    - beeline  접속하기 위한 추가 작업
    <br> 1) [hadoop@nn01 ~]$ cd $HADOOP_HOME/etc/hadoop
    <br> 2) [hadoop@nn01 hadoop]$ vi core-site.xml
    ~~~shell
    <property>
    <name>hadoop.proxyuser.hadoop.groups</name>
    <value>*</value>
    </property>
    <property>
    <name>hadoop.proxyuser.hadoop.hosts</name>
    <value>*</value>
    </property>
    ~~~
    <br> 3) [hadoop@nn01 hadoop]$ scp core-site.xml hadoop@dn01:/opt/hadoop/current/etc/hadoop
    <br> 4) [hadoop@nn01 hadoop]$ scp core-site.xml hadoop@dn02:/opt/hadoop/current/etc/hadoop
    <br> 5) [hadoop@dn01 ~]$ hdfs dfs -chmod -R 777 /tmp
    <br> 6) [hadoop@dn01 ~]$ hdfs dfs -chmod -R 777 /user/hive/warehouse

* Hive 실행
    - 상태확인
    <br> 1) [hadoop@dn01 ~]$ systemctl startus mariadb.service

    - 시작
    <br> 2) [hadoop@dn01 ~]$ systemctl start mariadb.service

    - 메타스토어 실행 및 확인(백그라운드 실행)
    <br> 1) [hadoop@dn01 ~]$ hive --service metastore &
    <br> 2) [hadoop@dn01 ~]$ ps -ef | grep metastore

    - 하이브서버 구동(백그라운드 실행)
    <br> 1) [hadoop@dn01 ~]$ hive --service hiveserver2 &

* CLI : hive
    - 접속 및 sample 데이터베이스 생성
    <br> 1) [hadoop@dn01 ~]$ hive
    <br> 2) hive (default)> show databases;
    <br> 3) hive (default)> create database sample1;
    <br> 4) hive (default)> show databases;
    <br> 5) hive (default)> use sample1;
    <br> 6) hive (sample1)> create table employees( name String );
    <br> 7) hive (sample1)> insert into employees(name) values('KIM');

    - 확인(테이블은 디렉터리, 값은 파일로 저장해준다.)
    <br> 1) [hadoop@dn02 ~]$ ssh dn01
    <br> 2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse
    <br> 3) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/sample1.db
    <br> 4) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/sample1.db/employees
    <br> 5) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/sample1.db/employees/000000_0
    <br> 6) hive (sample1)> !hdfs dfs -cat /user/hive/warehouse/sample1.db/employees/000000_0;
    <br> 7) hive (sample1)> !hdfs dfs -ls /user/hive/warehouse/sample1.db/employees;
    <br> 8) hive (sample1)> exit;

* CLI : beeline
    - 접속
    <br> 1) [hadoop@dn01 ~]$ beeline
    <br> 2) beeline> !connect jdbc:hive2://
    <br> 3) Enter username for jdbc:hive2://: hive
    <br> 4) Enter password for jdbc:hive2://: hive

    - sample데이터베이스 추가 및 확인
    <br> 1) 0: jdbc:hive2://> show databases;
    <br> 2) 0: jdbc:hive2://> use sample1;
    <br> 3) 0: jdbc:hive2://> show tables;
    <br> 4) 0: jdbc:hive2://> insert into employees(name) values('PARK');
    <br> 5) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/sample1.db/employees/000000_0_copy_1
    <br> 6) 0: jdbc:hive2://> select * from employees;

* 연습
    - kdatademo데이터베이스 생성
    <br> 1) 0: jdbc:hive2://> create database kdatademo;
    <br> 2) 0: jdbc:hive2://> show databases;
    <br> 3) 0: jdbc:hive2://> use kdatademo;
    <br> 4) 0: jdbc:hive2://> create table customers(id bigint, name string, address string);
    <br> 5) 0: jdbc:hive2://> show tables;
    <br> 6) 0: jdbc:hive2://> desc customers;
    <br> 7) 0: jdbc:hive2://> insert into customers values
    <br> 8) . . . . . . . . > (1111, 'kim', 'WA'),
    <br> 9) . . . . . . . . > (2222, 'park', 'WA'),
    <br> 10) . . . . . . . . > (3333, 'lee', 'WA'),
    <br> 11) . . . . . . . . > (4444, 'meng', 'CA'),
    <br> 12) . . . . . . . . > (5555, 'bae', 'NJ'),
    <br> 13) . . . . . . . . > (6666, 'jeon', 'NY');

    - 확인
    <br> 1) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hive/warehouse/kdatademo.db/customers
    <br> 2) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/kdatademo.db/customers/000000_0

    - 생성
    <br> 1) 0: jdbc:hive2://> create table if not exists orders(
    <br> 2) . . . . . . . . > id bigint,
    <br> 3) . . . . . . . . > product_id string,
    <br> 4) . . . . . . . . > customer_id bigint,
    <br> 5) . . . . . . . . > quantity int,
    <br> 6) . . . . . . . . > amount double);
    <br> 7) 0: jdbc:hive2://> insert into orders values
    <br> 8) . . . . . . . . > (111111, "phone", 1111, 3, 1200),
    <br> 9) . . . . . . . . > (222222, "camera", 1111, 1, 5200),
    <br> 10) . . . . . . . . > (100003, "notebook", 1111, 1, 10),
    <br> 11) . . . . . . . . > (100004, "bag", 2222, 1, 4500),
    <br> 12) . . . . . . . . > (100005, "t-shirt", 4444, 30, 50);

    - 주소가 "WA"인 고객 검색
    <br> 0: jdbc:hive2://> select * from customers where address="WA";

    - 주소가 "WA"이면서 id가 2222 보다 큰 고객 명단 출력
    <br> 0: jdbc:hive2://> select * from customers where address="WA" AND id>2222;
    
    - 주소로 정렬하여 고객명과 주소 출력
    <br> 0: jdbc:hive2://> select name, address from customers order by address;

    - 고객 명단 수 검색
    <br> 0: jdbc:hive2://> select count(*) from customers;

    - 첫번째 고객명단 출력(mysql 명령어 이용)
    <br> 0: jdbc:hive2://> select * from customers limit 1;

    - 주소별 인원수 검색
    <br> 0: jdbc:hive2://> select address, count(*) cnt from customers group by address;

    - 고객아이디, 고객명과 고객이 주문한 상품명 출력
    <br> select c.id, c.name, o.product_id from customers c left outer join orders o on (c.id=o.customer_id);

* mobilephones
    - 생성
    <br> 1) [hadoop@dn01 ~]$ mkdir /home/hadoop/hive_data
    <br> 2) [hadoop@dn01 ~]$ vi /home/hadoop/hive_data/mobilephones.csv
    ~~~shell
    Samsungg7,Samsung G7,250,red#white#blue,5.5,camera:true#dualsim:false
    Iphoneplus7,Iphone plus 7,400,gold#white,4.5,autofocus:true
    ~~~
    <br> 1) 0: jdbc:hive2://> create table mobilephones
    <br> 2) . . . . . . . . > (id string,
    <br> 3) . . . . . . . . > title string,
    <br> 4) . . . . . . . . > cost double,
    <br> 5) . . . . . . . . > colors array<string>,
    <br> 6) . . . . . . . . > screen_size array<float>,
    <br> 7) . . . . . . . . > features map<string, Boolean>
    <br> 8) . . . . . . . . > )
    <br> 9) . . . . . . . . > row format delimited fields terminated by ','
    <br> 10) . . . . . . . . > collection items terminated by '#'
    <br> 11) . . . . . . . . > map keys terminated by ':'
    <br> 12) . . . . . . . . > ;

    - 확인 
    <br> 1) 0: jdbc:hive2://> load data local inpath '/home/hadoop/hive_data/mobilephones.csv' into table mobilephones;
    <br> 2) 0: jdbc:hive2://> select * from mobilephones;
    <br> 3) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/kdatademo.db/mobilephones/mobilephones.csv

    - 제품아이디와 첫번째 색상 출력
    <br> 0: jdbc:hive2://> select id, colors[0] from mobilephones;

    - 제품명과 카메라특성 출력
    <br> 0: jdbc:hive2://> select id, features['camera'] from mobilephones;

* mobilephones2
    - 생성
    <br> 1) [hadoop@dn01 ~]$ cp ~/hive_data/mobilephones.csv ~/hive_data/mobilephones2.csv
    ~~~shell
    Samsungg7,Samsung G7,250,red#white#blue,5.5,camera:true#dualsim:false,24 hours#2MP
    Iphoneplus7,Iphone plus 7,400,gold#white,4.5,autofocus:true,2 hours#2MP
    ~~~
    <br> 2) 0: jdbc:hive2://> create table mobilephones2
    <br> 3) . . . . . . . . > (id string,
    <br> 4) . . . . . . . . > title string,
    <br> 5) . . . . . . . . > cost double,
    <br> 6) . . . . . . . . > colors array<string>,
    <br> 7) . . . . . . . . > screen_size array<float>,
    <br> 8) . . . . . . . . > features map<string, Boolean>,
    <br> 9) . . . . . . . . > infomation struct<battery:string,camera:string>
    <br> 10) . . . . . . . . > )
    <br> 11) . . . . . . . . > row format delimited fields terminated by ','
    <br> 12) . . . . . . . . > collection items terminated by '#'
    <br> 13) . . . . . . . . > map keys terminated by ':'
    <br> 14) . . . . . . . . > ;

    - 확인
    <br> 1) 0: jdbc:hive2://> load data local inpath '/home/hadoop/hive_data/mobilephones2.csv' into table mobilephones2;
    <br> 2) 0: jdbc:hive2://> select * from mobilephones2;
    <br> 3) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hive/warehouse/kdatademo.db/mobilephones/mobilephones2.csv

    - 제품아이디, 첫번째 색상, 카메라 특성, 배터리 정보 출력
    <br> 0: jdbc:hive2://> select id, colors[0], features['camera'], infomation.battery from mobilephones2;

* grouplens
    - 설치
    <br> 1) [root@dn01 ~]$ yum install unzip
    <br> 2) [hadoop@dn01 ~]$ cd ~/hive_data
    <br> 3) [hadoop@dn01 hive_data]$ wget http://www.grouplens.org/system/files/ml-100k.zip
    <br> 4) [hadoop@dn01 hive_data]$ unzip ml-100k.zip
    <br> 5) [hadoop@dn01 hive_data]$ cd ml-100k
    <br> 6) [hadoop@dn01 ml-100k]$ hdfs dfs -mkdir /user/hadoop/movies
    <br> 7) [hadoop@dn01 ml-100k]$ hdfs dfs -mkdir /user/hadoop/userinfo
    <br> 8) [hadoop@dn01 ml-100k]$ hdfs dfs -put /home/hadoop/hive_data/ml-100k/u.item /user/hadoop/movies
    <br> 9) [hadoop@dn01 ml-100k]$ hdfs dfs -put /home/hadoop/hive_data/ml-100k/u.user /user/hadoop/userinfo
    
## 정리
* Hive
    - 하둡에서 동작하는 데이터 웨어하우스 인프라 구조로서 데이터 요약, 질의 및 분석 기능을 제공