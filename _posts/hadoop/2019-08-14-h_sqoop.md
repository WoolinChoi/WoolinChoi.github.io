---
layout: post
title: sqoop
category: hadoop
tags: [hadoop, sqoop]
comments: false
---

# [sqoop]()

## 학습
* sqoop import, export
* sqoop-hive 연결
* spark 설치

## 실습
* mysql
    - 접속
~~~
1) [hadoop@dn02 ~]$ ssh dn01
2) [hadoop@dn01 ~]$ systemctl status mariadb.service
3) [hadoop@dn01 ~]$ mysql -u root -p
~~~
    
    - sqoopdemo 테이블 생성
~~~
1) MariaDB [(none)]> show databases;
2) MariaDB [(none)]> create database sqoopdemo;
3) MariaDB [(none)]> use sqoopdemo;
4) 테이블 생성
create table departments(
department_id int(11) unsigned NOT NULL,
department_name varchar(30) NOT NULL,
PRIMARY KEY (department_id));
5) 입력
INSERT INTO departments (department_id, department_name)
VALUES
(1, 'finess'),
(2, 'sportware'),
(3, 'appare'),
(4, 'gold'),
(5, 'outdoor'),
(6, 'fat shop');
6) MariaDB [sqoopdemo]> select * from departments;
~~~

    - sqoop에서 mysql 연결 확인
~~~
1) [hadoop@dn01 ~]$ sqoop list-databases --connect jdbc:mysql://dn01 --username hive --password hive
2) [hadoop@dn01 ~]$ sqoop list-tables --connect jdbc:mysql://dn01/sqoopdemo --username hive --password hive
~~~

    - sqoop을 통해 mysql -> hdfs(import) 확인
~~~
1) [hadoop@dn01 ~]$ sqoop import --connect jdbc:mysql://dn01/sqoopdemo --table departments --username hive --password hive --target-dir sqoopdemo
2) [hadoop@dn01 ~]$ hdfs dfs -ls /user/hadoop/departments
3) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hadoop/departments/part-m-*
~~~

    - sqoop을 통해 hdfs -> mysql(export) 확인
~~~
1) MariaDB [sqoopdemo]> create table dept like departments;
2) MariaDB [sqoopdemo]> select * from dept;
3) [hadoop@dn01 ~]$ sqoop export --connect jdbc:mysql://dn01:3306/sqoopdemo --table dept --username hive --password hive --export-dir departments
4) MariaDB [sqoopdemo]> select * from dept;
~~~

    - 추가하기(import)
~~~
1) MariaDB [sqoopdemo]> insert into departments values(7, 'it items');
2) MariaDB [sqoopdemo]> select * from departments;
3) sqoop export --connect jdbc:mysql://dn01:3306/sqoopdemo --table dept --username hive --password hive --export-dir departments
4) MariaDB [sqoopdemo]> insert into departments values(8, 'food');
5) MariaDB [sqoopdemo]> select * from departments;
6) [hadoop@dn01 ~]$ sqoop import --connect jdbc:mysql://dn01/sqoopdemo --table departments --username hive --password hive --check-column department_id --incremental append --last-value 7
7) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hadoop/departments/part-m-*
8) MariaDB [sqoopdemo]> insert into departments values(9, 'milk');
9) [hadoop@dn01 ~]$ sqoop import --connect jdbc:mysql://dn01/sqoopdemo --table departments --username hive --password hive --check-column department_id --incremental append --last-value 7
10) [hadoop@dn01 ~]$ hdfs dfs -cat /user/hadoop/departments/part-m-*
~~~

    - 추가하기(export)
~~~
1) [hadoop@dn01 ~]$ sqoop export --connect jdbc:mysql://dn01:3306/sqoopdemo --table dept --username hive --password hive --export-dir departments;
2) MariaDB [sqoopdemo]> select * from dept;
3) [hadoop@dn01 ~]$ sqoop export --connect jdbc:mysql://dn01:3306/sqoopdemo --table dept --username hive --password hive --export-dir departments --update-key department_id --update-mode allowinsert
~~~

* hive
    - 접속
~~~
1) [hadoop@dn01 ~]$ hive --service metastore &
2) [hadoop@dn01 ~]$ hive --service hiveserver2 &
3) [hadoop@dn01 ~]$ hive
4) hive (default)> use kdatademo
4) hive (kdatademo)> show tables;
~~~
    
    - hive 가져오기
~~~
1) [hadoop@dn01 ~]$ hdfs dfs -rm -r /user/hadoop/departments
2) [hadoop@dn01 ~]$ sqoop import --connect jdbc:mysql://dn01:3306/sqoopdemo --username hive --password hive --table departments --hive-import --hive-table kdatademo.departments -m 1
3) winSCP로 hive-common-0.10.0.jar파일 /opt/sqoop/1.4.7/lib로 옮기기
4) [hadoop@dn01 ~]$ hdfs dfs -rm -r /user/hadoop/departments
5) [hadoop@dn01 ~]$ sqoop import --connect jdbc:mysql://dn01:3306/sqoopdemo --username hive --password hive --table departments --hive-import --hive-table kdatademo.departments -m 1
6) hive (kdatademo)> show tables;
7) hive (kdatademo)> select * from departments;
~~~

* spark 
    - 설치
~~~
1) [root@dn01 ~]# cd /tmp
2) [root@dn01 tmp]# wget http://apache.mirror.cdnetworks.com/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz
3) [root@dn01 tmp]# tar xzvf spark-2.3.3-bin-hadoop2.7.tgz
4) [root@dn01 tmp]# mkdir -p /opt/spark/2.3.3
5) [root@dn01 tmp]# mv spark-2.3.3-bin-hadoop2.7/* /opt/spark/2.3.3/
6) [root@dn01 tmp]# ln -s /opt/spark/2.3.3 /opt/spark/current
7) [root@dn01 tmp]# chown -R hadoop:hadoop /opt/spark/
~~~

    - 환경변수
~~~shell
1) [hadoop@dn01 ~]$ vi ~/.bash_profile
###### spark  ######################
export SPARK_HOME=/opt/spark/current
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
#### spark ######################
2) [hadoop@dn01 ~]$ source ~/.bash_profile
3) [hadoop@dn01 ~]$ cd $SPARK_HOME/conf 
4) [hadoop@dn01 conf]$ cd $SPARK_HOME/conf 
5) [hadoop@dn01 conf]$ mv slaves.template slaves
6) [hadoop@dn01 conf]$ vi slaves
########## slaves ###########
nn01
dn01
dn02
########## slaves ###########
7) [hadoop@dn01 conf]$ mv spark-defaults.conf.template spark-defaults.conf
8) [hadoop@dn01 conf]$ vi spark-defaults.conf
########## spark-defaults.conf ##########
spark.yarn.jars /opt/spark/current/jars/*
########## spark-defaults.conf ##########
9) [hadoop@dn01 conf]$ mv log4j.properties.template log4j.properties
10) [hadoop@dn01 conf]$ vi log4j.properties
########## log4j.properties ###########
log4j.rootCategory=ERROR, console
########## log4j.properties ###########
11) [hadoop@dn01 conf]$ mv spark-env.sh.template spark-env.sh
12) [hadoop@dn01 conf]$ vi spark-env.sh
########### spark-env.sh ###########
SPARK_MASTER_HOST=dn01
export JAVA_HOME=/opt/jdk/current
export HADOOP_HOME=/opt/hadoop/current
export SPARK_HOME=/opt/spark/current
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_DRIVER_MEMORY=10g
export SPARK_EXECUTOR_INSTANCES=2
export SPARK_EXECUTOR_CORES=3
export SPARK_EXECUTOR_MEMORY=10g
#export SPARK_WORKER_DIR=/spark_data/spwork
#export SPARK_PID_DIR=/spark_data/sptmp
export SPARK_DIST_CLASSPATH=$(/opt/hadoop/current/bin/hadoop classpath):/opt/spark/current/jars/*
#export PYTHONPATH=/opt/python/current/python3
#export PYSPARK_PYTHON=/opt/python/current/python3
########### spark-env.sh ###########
~~~

    - 접속
~~~
1) [hadoop@dn01 ] spark-shell
2) scala> sc.setLogLevel("WARN")
3) scala> val f = sc.textFile("file:///etc/hosts")
4) scala> f.count
5) scala> f.first
6) scala> f.collect
7) scala> :quit
~~~

    - dn02 계정 설정
~~~shell
1) [hadoop@dn01 conf]$ sudo scp -r /opt/spark  dn02:/opt/spark
2) [hadoop@dn02 ~]$ ll /opt/spark
3) [hadoop@dn02 ~]$ su - root
4) [root@dn02 ~]# rm -rf /opt/spark/current
5) [root@dn02 ~]# ln -s /opt/spark/2.3.3 /opt/spark/current
6) [root@dn02 ~]# ll /opt/spark/
7) [root@dn02 ~]# chown -R hadoop:hadoop /opt/spark/
8) [root@dn02 ~]# su - hadoop
9) [hadoop@dn02 ~]$ vi ~/.bash_profile
###### spark  ######################
export SPARK_HOME=/opt/spark/current
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
#### spark ######################
10) [hadoop@dn02 ~]$ source ~/.bash_profile
~~~
    
    - nn01 계정 설정
~~~shell
1) [hadoop@dn01 conf]$ sudo scp -r /opt/spark  nn01:/opt/spark
2) [hadoop@nn01 ~]$ ll /opt/spark
3) [hadoop@nn01 ~]$ su - root
4) [root@nn01 ~]# rm -rf /opt/spark/current
5) [root@nn01 ~]# ln -s /opt/spark/2.3.3 /opt/spark/current
6) [root@nn01 ~]# ll /opt/spark/
7) [root@nn01 ~]# chown -R hadoop:hadoop /opt/spark/
8) [root@nn01 ~]# su - hadoop
9) [hadoop@nn01 ~]$ vi ~/.bash_profile
###### spark  ######################
export SPARK_HOME=/opt/spark/current
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
#### spark ######################
10) [hadoop@nn01 ~]$ source ~/.bash_profile
~~~

    - 확인
~~~shell
1) [hadoop@dn01 conf]$ cd $SPARK_HOME/sbin
2) [hadoop@dn01 sbin]$ pwd
3) [hadoop@dn01 sbin]$ ll
4) [hadoop@dn01 sbin]$ ./start-all.sh
5) [hadoop@dn02 sbin]$ jps
########## jps ###########
30473 Worker
########## jps ###########
~~~

    - 실행
~~~
1) [hadoop@dn01 sbin]$ spark-shell (로컬)
2) [hadoop@dn01 sbin]$ spark-shell --master spark://dn01:7077 (spark클러스트에서 실행)
3) http://192.168.56.102:4040/jobs
4) http://192.168.56.102:8080
5) scala> val x = sc.parallelize(List("spark", "example", "spark", "spark"), 3)
6) scala> val y = x.map(x=>(x,1))
7) scala> y.collect
8) scala> :quit
9) [hadoop@dn01 sbin]$ pyspark
10) >>> x = sc.parallelize(("spark", "example", "spark", "spark"), 3)
11) >>> y = x.map(lambda x:(x,1))
12) >>> y.collect()
13) >>> quit()
~~~

## 정리
* export는 DDL(테이블 구조)가 미리 있어야 함
